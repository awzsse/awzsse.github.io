---
layout:     post
title:      大白话机器学习算法-感知机
subtitle:   感知机听得懂的版本
date:       2020-01-22
author:     Noah-Yiliu
header-img: img/post-bg-2015.jpg
catalog: true
tags:
    - Machine Learning
---

# 写在前面
从**地质**转到**数据科学**，其中的艰辛不难得知。作为本专业的核心技术，机器学习的各种算法是必经的槛。结合李航老师的《统计学习方法》以及大神[Dodo](https://www.pkudodo.com/)的算法解析，运用最通俗简单的语言总结最主流的机器学习方法。

话不多说，第一道菜-感知机。

# 感知机
## 什么是感知机-直观解释
作为《统计学习方法》的第一个算法，他一定是最简单直观的。既然西瓜书用西瓜🍉来举例，我们就用猕猴桃🥝（因为我一直对惠康的猕猴桃有着蜜汁热爱）。假设我们需要选择一批猕猴桃中较为好吃的，那么我们用什么评判标准呢？一般来说用果蒂，毛色等可以对其进行很好的区分，那么这里的果蒂，毛色也就是机器学习中的**特征**。如下图所示，我们把果蒂作为横轴，毛色作为纵轴可以做出有限样本的图：

![是不是好猕猴桃呢？](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1579260047041&di=a3d2b87b9bc12a16fd412fae5ce5e8a7&imgtype=0&src=http%3A%2F%2Fww2.sinaimg.cn%2Flarge%2F7cc829d3gw1eupagykblaj20ff0fddhc.jpg)

假设图中红色的点是好吃的猕猴桃，蓝色的点是没那么好吃的那些，我们可以通过画一条“分界线”将其分开，这里这条线就是机器学习中的**“分类超平面”**，由于这是个二维数据，因此是一条直线。

但是这条直线该怎么画出来呢。换句话说，我们的电脑怎么知道要画出这条直线呢？在这里，我们要假设所有的数据是**线性可分**的，什么叫线性可分呢？根据大白话定义：

>对于某个数据集，如果存在一个超平面（之前提到过）可以将其完全分开，那么这个数据集就是**线性可分**的。

在这里，如果猕猴桃的点符合**线性可分**，那么我一定能画出一条线把好坏猕猴桃分开。对于计算机来说，我们只需要考虑的是误分类点，也就是我们需要尽量减少**“好坏不分”**。具体的，我们给好的猕猴桃赋值为+1，坏的猕猴桃赋值为-1，这个值称为**"标签"**。那么如果我们把误分类点到分类超平面的距离尽量减到最小也就是零（没有任何一个点被误分类），那么所有的点都被正确分类了，这个划分平面也就随即得出了。

## 感知机的实现
因为本博客希望用最简单的语言总结机器学习算法，我们会省略大部分的数学推导过程（**CSDN**大神很多）。回归到感知机的实现，我们知道大部分机器学习算法的求解过程其实就是一个**最优化问题**，那么优化的目标函数一般来说就是[损失函数](https://zhuanlan.zhihu.com/p/58883095)。之前我们提到，为什么我希望的是减小误分类点到平面的距离，而不是直接减少误分类点的个数，这样不是更直观吗？对，因为距离是连续的可导函数，我们可以使用**梯度下降**的方法对其进行优化（关于**梯度下降**可以参考[这里](https://zhuanlan.zhihu.com/p/31630368)）。简单说来，我们通过建立一个目标函数，对其进行梯度下降找找到最小值也就是最优解，也就是让损失函数达到最小值（误分类点最少，这里为零），从而得出使得目标函数最小的参数值，进而画出这个分离超平面，是不是思路清晰很多了。

### 损失函数
根据书上P37页的定义以及推导，最终的损失函数定义为<img src="https://latex.codecogs.com/gif.latex?L(w,b)=-\sum_{x_{i}\in&space;M}y_{i}(w·x_{i}&plus;b)" title="L(w,b)=-\sum_{x_{i}\in M}y_{i}(w·x_{i}+b)" />这里的**M**表示误分类点的集合，<img src="https://latex.codecogs.com/gif.latex?w,b" title="w,b" />为我们需要最终得到的参数。

### 梯度下降
利用**随机梯度下降**算法，每次选取一个**误分类点**进行梯度下降。也就是沿着梯度的反方向去更新参数，可以使得损失函数的值不断减小，根据计算，参数更新的公式定为<img src="https://latex.codecogs.com/gif.latex?w\leftarrow&space;w&plus;\eta&space;y_{i}x_{i}$以及$b\leftarrow&space;b&plus;\eta&space;y_{i}" title="w\leftarrow w+\eta y_{i}x_{i}$以及$b\leftarrow b+\eta y_{i}" />。这里<img src="https://latex.codecogs.com/gif.latex?\eta" title="\eta" />为**学习率**，可以自己定义大小，指示着学习到最优参数的速度。但是需要注意的是，过大的学习率可能导致算法无法收敛，具体可以参考[这里](https://www.cnblogs.com/HuZihu/p/10830776.html)。

最终，当所有点的<img src="https://latex.codecogs.com/gif.latex?y_{i}(w·x_{i}&plus;b)\leq&space;0" title="y_{i}(w·x_{i}+b)\leq 0" />，即没有误分类点存在，停止更新参数，得到最优参数。经计算，该算法下，参数更新的次数收敛于某个值（书本P43）

### 对偶形式

实际上，感知机的实现还有一种对偶形式，其基本原理是将<img src="https://latex.codecogs.com/gif.latex?w,b" title="w,b" />表示为实例x和标记y的形式。同时利用[Gram矩阵](https://blog.csdn.net/lilong117194/article/details/78202637)，可以显著的提高运行速度，这里不做讨论。

# 写在最后
感知机的核心内容其实就是这些，是不是觉得机器学习也没想象中那么神秘了？后面的算法有些会在感知机的基础上继续深入，有些则是用不同的角度去理解问题，相关算法的python实现可以参考Dodo大神的博客（前文有提到），我会持续更新下去的！

*本文使用了codecogs作为数学公式的输入方法，其原理是直接生成链接格式的图片*
