---
layout:     post
title:      大白话决策树
subtitle:   爱护环境，人人种树
date:       2020-02-20
author:     Noah-Yiliu
header-img: img/taitou.jpg
catalog: true
tags:
    - Machine Learning
---
# 大白话决策树
## 写在前面
本以为决策树写起来挺快的，但没想到我光理解书上内容就花了不少时间。总体来说决策树包含***特征选择，决策树生成以及决策树的剪枝几个方面，其算法包括ID3，C4.5算法以及CART算法***。耗费时间最长的地方就是在***CART剪枝***部分，这个部分我也会好好地跟大家做出讨论。

## 决策树的基本理解
“决策树是一种基本的**分类**以及**回归**方法。”------李航


完。

咳咳，什么是决策树呢。相信大家都玩过文字类的剧情游戏，比如给你一系列的选项，简单一些的每次只给你**两个选项是或者否**，然后最终你会一步步的走向属于你的结局。其实决策树与之类似。猕猴桃警告⚠️假设我们有一群猕猴桃，那么关于他们的一颗简易版决策树可能会是下面的亚子：

![Markdown](http://i2.tiimg.com/708998/914ce7ad9ae20a36.png)

这里我们选择了毛色以及硬度这两个特征（**特征选择**），并且生成了一个决策树（**决策树生成**）。那么这些过程是怎么决定出来的，每一步又有什么样的评判标准呢，待我细细道来。

## 决策树的特征选择
那么什么叫特征选择嘞。假设我们上面的决策树不是只有这两个特征的，还有比如重量，长度等指标，但是我们为什么没有用到呢？因为**利用这些指标对猕猴桃进行分类的结果与随机分类的结果并没有很大差别，也就是说这个特征是没有分类能力的**。因此我们扔掉了这些指标，这个过程就叫做**特征选择**。通常我们使用**信息增益**或者**信息增益比**来作为特征选择的准则。

### 信息增益
什么叫信息增益呢，它为什么可以作为特征选择的准则呢？这里我们需要给出两个定义，也就是用来**衡量随机变量不确定性**的两个概念，首先是**熵**：
> 熵：表示随机变量不确定性的度量。假设X是一个取有限值的随机变量，其概率分布为
> $$P(X=x_{i})=p_{i}, i=1,2,3..,n$$
> 那么随机变量的熵为：
> $$H(X)=-\sum_{i=1}^{n}p_{i}logp_{i}$$

我们来解释一下，当熵越大的时候，说明随机变量的不确定性越大。那么在决策树中怎么体现呢？对，比如我这群猕猴桃全是好吃的，那么他的熵就0，因为我的随机变量只有一个值，他的概率就是1，代入公式就是0。那么如果正好是一半好吃一半不好吃呢？那么他的熵就最大，也就是说最混乱（这里证明就省略了，其实就是求最大值问题）。

另一个概念是**条件熵**：
> 条件熵：在已知随机变量X的条件下随机变量Y的不确定性。假设X是一个取有限值的随机变量，其概率分布为
> $$P(X=x_{i})=p_{i}, i=1,2,3..,n$$
> 那么已知随机变量X的条件下随机变量Y的条件熵为：
> $$H(Y|X)=-\sum_{i=1}^{n}p_{i}H(Y|X=x_{i})$$

简单点说就是针对某个特征$x_{i}$，我们求出这个特征取不同值的时候随机变量Y的熵然后加和，这就是条件熵。

好了有这两个概念以后，我们就可以得到信息增益的定义了：
> 特征A对于数据集D的信息增益定义为集合D的经验熵（概率由数据统计得到的）和特征A给定条件下D的经验条件熵H(D|A)的差，也就是$$g(D|A) = H(D) - H(D|A)$$

换句话说，选择某个特征以后，如果他的信息增益是最大的，就等于使得整个模型的混乱度减少程度也是最大的，那当然是最好的分类特征咯。

那么如何通过信息增益来**选择特征**呢？这里我们不去列出繁杂的数学表达（因为李航老师书上都有），而是用通俗的语言去解释。首先我们怎么求$H(D)$，我们假设这里有10个猕猴桃样本作为D，其中有5个好吃的，5个不好吃的，那么根据经验熵的定义，我们可以求出：

$$H(D) = -\frac{5}{10}log_{2}\frac{5}{10}-\frac{5}{10}log_{2}\frac{5}{10}$$

其次我们选择**毛色**这个特征作为A1，其中毛色均匀的猕猴桃有5个，其中3个是好吃的，而毛色不均匀的猕猴桃有5个，其中只有1个是好吃的，那么对应的关于A的数据D的经验条件熵为：

$$H(D|A_{1}) = -\frac{5}{10}log_{2}\frac{3}{5}-\frac{5}{10}log_{2}\frac{1}{5}$$

最后通过两者的差就能求出信息增益的值，是不是很直白？我们只要依次针对所有的特征计算他们单独的信息增益，选出**最大**的就是最有特征咯。

### 信息增益比
那么什么是信息增益比，搞这么多概念干啥？同学们，存在即合理，我们假设一种情况，也就是某个特征的取值数量太多（比如毛色有10种可能的取值，然后我的样本也只有10个），那么这样一来我选择这个特征作为最优特征的可能就会很大对不对（公式中条件熵计算很多次，而每个log会产生很大的值），那怎么避免这种情况发生呢？当然就是**信息增益比**咯~

> 信息增益比：
> $$g_{R}(D, A)=\frac{g(D, A)}{H_{A}(D)}$$
> 这里的$H_{A}(D)=-\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}log_{2}\frac{|D_{i}|}{|D|}$，n是特征A取值的个数。

## 决策树的生成
那么既然有了以上的特征选择方法，我们怎么去生成一颗决策树呢？首先我们来看看ID3算法。

### ID3算法
ID3算法就是利用**信息增益**这个指标来决定决策树生成的算法。同样，具体的算法解释书上都有，我们这里用文章刚开始的那个**决策树图**的例子帮助大家理解。

1. 假设我们有10个猕猴桃，我们通过计算特征的**信息增益**值发现**毛色**是**最优特征**，那么我们首先将**毛色**作为**根节点特征**。为了简单起见，我们假设毛色均匀为“是”的都是好吃的猕猴桃（复习，这里的熵最低为零），那么这个节点就成为了叶子节点，不在继续延伸。
2. 另一边毛色均匀为“否”，那么我们继续针对**其他特征**，也就是**剩余样本**中除去“毛色”特征的其他特征进行选择，假设这时**最优特征二号**变成了**硬度适中**，那么我们发现在这个特征下有3个硬度适中为“是”的都是好吃的，剩下两个是不好吃的，那么这两个延伸的节点都是叶子节点，完毕。

这样就会形成文章刚开始出现的那个**决策树**图，非常简单。

### C4.5算法
C4.5算法就是在ID3算法的基础上，将**信息增益**变成**信息增益比**。

## 决策树的剪枝
前面提到的方法帮我们生成了一颗**非常完整**的决策树，怎么完整个法呢？就是对**训练数据集**的**拟合程度非常高**，这里可能会带来**过拟合**的问题，从而导致在**测试集**上评分很低。那么怎么消除这种影响呢。辛勤的园丁告诉你，你可以在对这颗参天大树做一些修剪工作，英文为***pruning***。首先我们为决策树定义一个**损失函数**或者**代价函数**（cost function），假设树T的叶节点个数为$|T|$，t是某个叶节点，该叶节点有$N_{t}$个样本点，其中k类的样本点有$N_{tk}$个，$\alpha$为某个大于等于0的参数，那么损失函数定义为：

$$C_{\alpha}(T)=\sum_{t=1}^{|T|}N_{t}H_{t}(T)+\alpha|T|$$

其中经验熵为：

$$H_{t}(T)=-\sum_{k}\frac{N_{tk}}{N_{t}}log\frac{N_{tk}}{N_{t}}$$

所以在损失函数等式右边的第一项可以理解为模型对训练数据的**预测误差**，而第二项表示模型的**复杂度**，反应叶节点的数量。而$\alpha$控制了两者之间的平衡。如果$\alpha$越大，说明模型复杂度对整个损失函数影响更大，我会选择更为简单的模型，$\alpha$越小，我更多考虑训练数据的拟合情况。

而所谓的剪枝就是**假设$\alpha$确定**，从每个叶节点向上递归的回缩，并且比较每次回缩（剪枝）前后的整体损失函数，如果损失函数减小那么执行剪切，讲该叶节点的父节点变成新的叶节点，知道不能继续为止。CART中有更详细的步骤。

## CART算法
终于要到重头戏了，这个算法废了老半天时间终于弄懂了。它全名叫做**分类与回归树模型（classification and regression tree）**，也就是说他既可以做分类，也可以做回归，岂不美哉。同时其**假设决策树是二叉树**（是否），算法也主要包括**生成**和**剪枝**两个部分。

### CART回归树生成（简略，主要是分类树的描述）
我们反过来思考，首先假设这个树已经形成了，那么他一定将输入空间划分成了m个单元，每个单元$R_{m}$都有一个固定的输出值$c_{m}$。那么我们可以用**平方误差**$\sum_{x_{i}\in R_{m}}(y_{i}-f(x_{i}))^2$来表示回归树中训练数据的预测误差，通过最小化**平方误差**就可以找到每个单元的最优输出值，可以证明最优输出值就是在这个单元的**所有输出的均值**。

那么如果正过来思考的话，我们如何对空间进行划分呢。这里使用一种启发式算法，我们循环遍历所有的变量，针对每个变量$j$，我们尝试找到一个切分点$s$从而最小化切分点两边单元里的**最小平方误差**之和（由于**最小平方误差**在给定单元是可以直接计算的）。这样我们就得到了多对$(j, s)$，找出最优的$(j, s)$对将整体划分成两个单元，并且分别在这两个单元重复以上步骤。

这种算法称为**最小二乘回归树算法**。

### CART分类树的生成
#### 基尼指数
我们之前介绍了熵，代表了随机变量的不确定性，在CART中我们又出现了新的概念也就是**基尼指数**，他与熵的概念有一些神似，也是代表了一种混乱度，具体定义如下：
> 概率分布的基尼指数：$$Gini(p)=\sum_{k=1}^Kp_{k}(1-p_{k})=1-\sum_{k=1}^Kp_{k}^2$$

这代表着，如果在分类问题中有K个类，样本属于第k类的概率为$p_{k}$，可以看到，如果某个样本群体只有一个类别的时候，其基尼指数为零。

相对应的，我们也学过了条件熵，同样，在某特征A存在的前提下，集合D的基尼指数定义为：
> $$Gini(D, A)=\frac{|D_{1}|}{|D|}Gini(D_{1})+\frac{|D_{2}|}{|D|}Gini(D_{2})$$

#### 生成算法
有了基尼指数，等于有了生成判断标准。在ID3算法中是根据**信息增益最大的**来决定最优特征，这里是根据不同特征$A_{x}$的不同取值$a_{x}$选择可以使得**基尼指数最小的**即可，遵循不同的停止条件决定是否停止算法计算。

### CART剪枝

在决策树的算法中，CART剪枝可能是最不好理解的一个部分，复习之前**决策树剪枝**所提到的，我们通过某个确定的$\alpha$可以找到对应的**最优子树**。那么问题在哪里呢？$\alpha$是无限的数，可以取任何不小于零的值，同时随着$\alpha$等增大，决策树模型会趋向于简单，但是我们的$T_{\alpha}$是有限的，也就是剪枝形成的子树个数是有限的，这肯定是不好对应的。所以我们反过来思考，通过**有限个数的$T_{\alpha}$**去寻找对应的**$\alpha$**不就行了！这也是CART剪枝的核心思想。

那么假设我们从整体树$T_{0}$开始剪枝，其内部任意节点$t$在**剪枝前**的损失函数为：

$$C_{\alpha}(T_{t})=C(T_{t})+\alpha|T_{t}|$$

而在**剪枝后**其以t为叶节点（归并了其原先的两个叶节点）决策树的损失函数为：

$$C_{\alpha}(t)=C(t)+\alpha$$

这里等式右边的第一项均代表了对训练数据的测试误差，可以是基尼指数等。那么我们观察到，当$\alpha$很小的时候，模型复杂对对于损失函数的惩罚就很小，那么肯定是越复杂的模型损失函数越低，也就是：

$$C_{\alpha}(T_{t})<C_{\alpha}(t)$$

但是当$\alpha$逐步增加的时候一定会在某个时刻使得：

$$C_{\alpha}(T_{t})=C_{\alpha}(t)$$

也就是由于叶节点的减少，整体树的损失函数与剪切前相等了，但是由于子树的节点更少，因此overfitting的可能也就更小，更可取。这时，根据公式可以求得：

$$\alpha=\frac{C_{\alpha}(t)-C_{\alpha}(T_{t})}{|T_{t}|-1}$$

当然这只是针对某个内部节点t得出的，我们需要找到可以得到最小$\alpha$的$T_{t}$将其设为$\alpha_{1}$和$T_{1}$。这样循环剪枝直到根节点即可。

当我们得到这么多子树$T_{1},...T_{n}$后，只需要通过交叉验证的方法选出最优的作为最终决策树模型。

完

## 写在最后
监督学习最基本的几个方法已经介绍7788了，后期我们将继续学习**逻辑斯谛回归**以及**支持向量机**。





