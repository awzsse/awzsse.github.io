---
layout:     post
title:      大白话提升方法之AdaBoost
subtitle:   我们也要在逆境中提升自己
date:       2020-03-12
author:     Noah-Yiliu
header-img: img/taitou.jpg
catalog: true
tags:
    - Machine Learning
---
# 大白话提升方法之AdaBoost
## 前言
同志们好，经过之前的努力，我们已经学习完了一系列基础的机器学习算法。复习一下，从**感知机，k近邻，朴素贝叶斯，决策树，逻辑回归，SVM**以及**主成分分析方法PCA**等。那么我们就今天介绍的一种方法本质上是以简单的机器学习算法为单元，利用线性组合的方式创造出更为**强大**的模型。

## 集成学习
没错，我们提到的强大模型，就属于**集成学习**模型。简单介绍一下，集成学习分为两种，一种叫**bagging**，另一种叫**boosting**。一下子引入三个概念确实有点过分了坳。那我先说一下什么叫**bagging**。

大家应该都知道有放回抽样，那么理解**bagging**应该不会太难。我们每次从含有$n$个数据的数据集中抽出小于$m$个样本（$m<n$），训练出一个模型，再把这些数据**放回去**，进行下次抽样，这也称为**bootstrap**（自助法）。大家不难发现，我们每次这样操作的话，岂不是可以无穷无尽的进行抽样，得到无数个**bags**，再用这些**bags**训练出来的模型去作为一个整体去拟合数据。这个思路很正确，那么这么做的好处是什么呢？可以证明，样本被抽取的百分比会趋近于一个固定的值$1-\frac{1}{e}$，因此有大约$\frac{1}{3}$的样本不会被抽取到，一旦我们的模型中有很多噪音的话，这种方法可以很好的降低模型的方差。下图来自youtube教程，表示了两种集成学习方法的基本原理。


![](https://github.com/awzsse/awzsse.github.io/blob/master/img/Boosting.jpeg?raw=true)


好说完了**bagging**，我们还是要把目光聚集到今天需要学习的**boosting**上来。到底他有什么特性呢？主流集成学习方法中，比较有名的三个**boosting**算法是**AdaBoost**，**GBDT**，**XGBoost**，我将着重介绍前面两种，最后一个由于是一个较为完整的模型结构，需要慢慢消化，我就不在这误导大家了，但是会给出一个大致的概念同时与前面两种模型做出比较。

## AdaBoost
我们一切的算法原理都基于：

> 一些列的弱可学习方法可以组合成一个强可学习方法。

也就是三个臭皮匠顶一个诸葛亮。我们如果通过训练出一些弱的分类器，就可以通过某种线性组合方式将其拼凑成一个强的分类器，达到我们更好分类数据的目的（回归也一样）。那么AdaBoost是怎么做的呢？首先，他不是利用有放回抽样这个方式，所以他并不能做到**并行**，而是**序列**更新。具体的，我们以**相同权重**对数据进行训练得到，第一个分类器得出了一些结果，我们找出数据中的误分类点，给他们赋予更多的**关注**，也就是增加他们的**权重**进行下一次训练，这样训练出来的一些列弱分类器，我们通过给与**分类器**的表现赋予相应的**分类器权重**，也就是**误分类率好**的我们给**大**一点的权重。这样一来，知道最后我们的误差率小于某个阈值的时候停止训练。

下面给出这个算法的具体步骤：

（1）初始化训练数据的权值均为$\frac{1}{N}$

（2）对于$m=0,1,2,...,M$，我们重复下列过程：

&#160;&#160;&#160;（a）使用具有权值分布$D_{m}$的数据训练，的到基本分类器$G_{m}$

&#160;&#160;&#160;（b）计算分类误差率，具体的：

$$e_{m}=\sum_{i=1}^{N}P(G_{m}(x_{i})\neq y_{i})=\sum_{i=1}^{N}w_{mi}I(G_{m}(x_{i})\neq y_{i})$$
&#160;&#160;&#160; 也就是说，分类误差率就等于是被当前$G_{m}$误分类样本的**权值之和**。

&#160;&#160;&#160;（c）计算分类器的系数：

$$\alpha_{m}=\frac{1}{2}\log\frac{1-e_{m}}{e_{m}}$$

&#160;&#160;&#160;（d）更新数据集的权值分布。

$$w_{m+1,i}=\frac{w_{m,i}}{Z_{m}}exp(-\alpha_{m}y_{i}G_{m}(x_{i})), i=1,2,...,N$$

&#160;&#160;&#160; 这里的$Z_{m}$是一个规范化因子，也就是：

$$Z_{m}=\sum_{i=1}^{N}w_{mi}exp(-\alpha_{m}y_{i}G_{m}(x_{i}))$$

&#160;&#160;&#160; 这样使得所有的权值之和仍为1，变成一个概率分布。同时我们注意到，如果将这个公式拆开，也就是对于正确分类的点$w_{m+1,i}=\frac{w_{mi}}{Z_{m}}e^{-\alpha_{m}}$，对于错误分类的点$w_{m+1,i}=\frac{w_{mi}}{Z_{m}}e^{\alpha_{m}}$，说明错误分类点的权值被放大了，正确的被缩小了。

（3）构建基本分类器的线性组合，得到最终分类器：

$$f(x)=\sum_{m=1}^{M}\alpha_{m}G_{m}(x)$$

$$G(x)=sign(f(x))$$

我们这里注意到，只是根据最终分类器的符号来判断，但是组合分类器的值可不一定是正1或者负1噢。

### AdaBoost的训练误差分析
AdaBoost最基本的性质是训练误差会随着学习过程不断减小。这个性质非常关键，同时针对最终学习器的训练误差也有一个**上界**，这说明我们可以得到一个有限误差的分类器，具体我们给出如下的证明。


![](https://github.com/awzsse/awzsse.github.io/blob/master/img/adaboostbound1.png?raw=true)


进一步的，我们还可以证明这个上届是呈指数递减的。这点非常吸引人。


![](https://github.com/awzsse/awzsse.github.io/blob/master/img/adaboostbound2.png?raw=true)


### AdaBoost是加法模型的一个特例
首先先来看一个概念，考虑一个加法模型（additive model）：

> $$f(x)=\sum_{m=1}^{M}\beta_{m}b(x;\gamma_{m})$$
> 这里我们看到$b$就是基函数，$\beta$为基函数的系数。

因此我们再给定训练数据集的情况下学习这个加法模型其实就是其损失函数的极小化问题。同时由于我们模型是不断累加的，因此我们需要做的是每次学习一个基函数以及其系数，从而优化以下损失函数：

$$\min_{\beta,\gamma}\sum_{i=1}^{N}L(y_{i},\beta b(x_{i};\gamma))$$

我们考虑到AdaBoost也是使用分布算法，每次去更新一个新的子分类器的系数以及数据集元素的权重，那么这两者之间是否存在着某种联系呢？答案是肯定的。我们给出以下定义，这个定义从损失函数的角度描述了这个算法的合理性。

> 定理：AdaBoost算法是前向分布算法的特例，这时模型是由基本分类器组成的加法模型，并且损失函数是指数函数：
> $$L(y,f(x))-exp[-yf(x)]$$

具体证明过程就不赘述了，也就是说根据定义得到的这个加法模型，其实就是我们前面针对AdaBoost做出的算法总结过程得到的模型，这样一来我们就可以从损失函数角度来合理化这个模型了。

### 提升树（BT）
下面说一个AdaBoost的应用，**提升树**。这也被认为是统计学习中性能最好的方法之一。定义也非常简单，就是以**决策树**为基函数的提升方法就称为**提升树**。我们假设已经得到了模型$f_{m-1}(x)$，那么下一步的模型为：

$$f_{m}(x)=f_{m-1}(x)+T(x;\Theta_{m})$$

那么这下一颗树的参数通过才能确定呢？这里我们选择经验风险极小化来决定。回想一下，之前说的AdaBoost算法的损失函数是一个指数函数，如果我们的基函数是一个**决策二叉分类树**的话，那么提升树问题就完全回归到之前说过的方法，也就是通过改变分类器和数据的权重来降低最终分类器的误差。但是如果这里是回归问题的话，我们需要引入新的方法。

由于之前我们已经在之前**决策树博客**中详细描述了如何一步一步的建立一个回归树，我们如何利用之前的知识来学习回归提升树模型呢？假设我们建立了某个回归树模型，其被分成了$J$个互不相交的区域，并且在每个区域上确定输出某个常量$c_{j}$。那么这个树可以表示成：

$$T(x;\Theta)=\sum_{j=1}^{J}c_{j}I(x\in R_{j})$$

那么套用前面的前向分布算法模型，我们需要最小化每一步的经验风险，即：

$$\hat\Theta=\argmin_{\Theta_{m}}=\sum_{i=1}^{N}L(y_{i},f_{m-1}(x_{i})+T(x;\Theta))$$

到了非常有意思的地方了。我们以往针对这种优化问题，经常使用梯度下降方法去更新参数，也就是说我们需要按照**损失函数**的**梯度下降**方向逐渐找到**损失函数的最小值**。那么这里，如果我们取损失函数为均方误差损失函数的时候：

$$L(y,f_{m-1}(x)+T(x;\Theta_{m}))=[y-f_{m-1}(x)-T(x;\Theta_{m})]^2=[r-T(x;\Theta_{m})]^2$$

这里的$r$表示我们当前这个模型拟合数据的残差。因此，我们如果不从梯度下降的方向考虑。这里最直觉的方法，就是让新加入的***基函数去拟合残差***！如果越接近，那岂不是我这个损失函数就越小。达到目的？因此我们列出基本算法步骤。

（1）我们利用决策树博客中提到的模型，去训练第一个回归树$T_{1}(x)$，针对每个变量$j$，我们尝试找到一个切分点$s$从而最小化切分点两边单元里的**最小平方误差**之和（由于**最小平方误差**在给定单元是可以直接计算的）。这样我们就得到了多对$(j, s)$，找出最优的$(j, s)$对将整体划分成两个单元。这样我们得到第一个树为：

$$T_{1}(x_{j})=\begin{cases} 
c_{1}& {x_{j}<0}\\
c_{2}& {x_{j}\geq1}
\end{cases}$$ 

$$f_{1}(x)=T_{1}(x)$$

（2）根据之前的分析，我们再利用已经得到的模型$f_{1}(x)$和$y_{i}$的残差作为拟合对象，得到第二棵分类树$T_{1}(2)$。

（3）等到平方损失误差达到一个阈值后，通过线性组合这些基函数得到最终分类器。

因此我们并不是通过去分配数据以及分类器以不同权重来更新最终强分类器了，而是直接根据降低损失函数的方法。这里我们留一个悬念放到后面要提到的**GBDT**中，那就是**拟合残差的合理性**，其实在后面要提到的**GBDT**中，这一点的到了统一，其实这本是关于**负梯度**的故事。


## 尾记
本想把GBDT以及XGBoost一起写了，但是发现不能这么草率的引入这么多内容。所以关于这两者的具体内容请大家翻阅我的下一篇博客。


