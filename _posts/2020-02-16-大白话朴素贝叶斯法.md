---
layout:     post
title:      大白话朴素贝叶斯法
subtitle:   我朴素而严谨
date:       2020-02-17
author:     Noah-Yiliu
header-img: img/taitou.jpg
catalog: true
tags:
    - Machine Learning
---

# 大白话朴素贝叶斯法
## 写在前面
终于要继续大白话机器学习系列了，今天我们要说的方法与我的前一篇博客有着密切关系，相信认真阅读过的同学可以轻松理解~

## 朴素贝叶斯法的“朴素”理解（警告，这次直接进入数学PAR）
什么叫朴素贝叶斯法？我们先不管朴素的问题，还记得[前一篇博客](https://awzsse.github.io/2020/02/16/%E5%A4%A7%E7%99%BD%E8%AF%9DMLE_MAP_%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F/)提到的**贝叶斯公式**还有**最大后验概率**吗？本质上朴素贝叶斯法就是通过最大后验概率这个方法实现的（不懂的同学请自行参考上篇博客）。假设X是定义在**输入空间**上的**随机向量**，Y是定义在**输出空间**上的**随机变量**。那么结合先前所学知识，$P(Y=c_{k})$称为先验概率，也就是数据集中不同分类类别出现的概率，$P(X=x|Y=c_{k})$称为条件概率，也就是该数据集中不同类别出现某个**特征x**的概率。具体来看这个条件概率：

$$P(X=x|Y=c_{k})=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...,X^{(n)}=x^{(n)}|Y=c_{k}), k=1,2...K$$

这里每个小x代表了不同维度的特征值，如果不作出一些前提假设是很难计算出这个条件概率的，因此**朴素**贝叶斯法对这个条件概率分布做出了条件独立性的假设，即

> 用于分类的特征在类确定的情况下都是条件独立的

这么拗口可不行，用人话说就是，每个特征之间没关系，那么整个条件概率就可以看做是每个特征对应的条件概率相乘：

$$P(X=x|Y=c_{k})=\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}), k=1,2...K$$

这样一来你给我数据我就是数也能数出来了不是么。

![Markdown](http://i1.fuimg.com/708998/8de94b69c46ca06e.jpg)

好了，拥有了先验概率以及条件概率，朴素贝叶斯方法通过计算后验概率并且将**后验概率最大**的类作为某个x的输出，该分类器可以表示为：

$$y=f(x)=argmax_{c_{k}}\frac{P(Y=c_{k})\prod_{j=1}^{n}P(X^{(j)}=x^{(j)})}{\sum_{k}P(Y=c_{k})\prod_{j=1}^{n}P(X^{(j)}=x^{(j)})}$$

一般来说，由于该式的分母在某个数据集内是固定的，因此我们需要最大化的只有分子部分：

$$y=f(x)=argmax_{c_{k}}P(Y=c_{k})\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}$$

到这里，分类器的模型基本搭建完了。有同学想问，一般不都是通过**最小化风险函数**来算的吗，比如感知机最小化的是误分类点的数量，k近邻法中是利用多数表决规则，本质上也是最小化经验风险函数来决定的。有这个疑问说明你是学到位了，其实这里最大化后验概率同样符合期望风险最小化的定义，具体可以参考统计学习方法的P61页，李航老师做出了详细的推导，由于并不影响我们理解后面的具体算法，就暂不列出了（我懒我承认）。

## 朴素贝叶斯的参数估计（也就是各种概率是怎么算的）
### 极大似然估计
是不是很熟悉，在复习一遍，极大似然估计就是我如何设定参数让某事件最可能发生。既然给定了我数据集，那么先验概率的极大似然估计就是：

$$P(Y=c_{k})=\frac{\sum_{i=1}^{N}I(y_{i}=c_{k})}{N}, k=1,2,...,K$$

这里又出现了熟悉的**指示函数**。不难理解，数据集出现的不同类别的概率可以当做我们的先验来输入。而对于条件概率，我们假设第j个特征可能取值的集合为$\{a_{j1},...,a_{jS_{j}}\}$，则极大似然估计为：

$$P(X^{(j)}=a_{jl}|Y=c_{k})=\frac{\sum_{i=1}^{N}I(x_{i}^{(j)}=a_{jl},y_{i}=c_{k})}{\sum_{i=1}^{N}I(y_{i}=c_{k})}$$

这里$l=1,2,...,S_{j}$,代表第j个特征能取值的个数。也就是说，每个分类类别下，每个特征在整个样本该特征维度下出现的次数与该类别出现总次数相比就是对应特征的**条件概率**值。我们最终将某x对应所有特征的条件概率值相乘就得到了总的条件概率。

### 贝叶斯估计
有同学又坐不住了，搞个极大似然估计不就完了，咋又来个贝叶斯估计。其实这玩意是很有用的，主要是针对所要估计的概率值为零的情况。具体定义如下：

> 朴素贝叶斯中，通过在在随机变量的各个频数取值上添加一个正数$\lambda$，作为排除概率值为0的方法。如果$\lambda=0$，就回归极大似然估计，如果$\lambda=1$，就称为**拉普拉斯平滑**

因此，对于先验概率的贝叶斯估计为：

$$P(Y=c_{k})=\frac{\sum_{i=1}^{N}I(y_{i}=c_{k})+\lambda}{N+K\lambda}$$

而对于条件概率的贝叶斯估计是：

$$P(X^{(j)}=a_{jl}|Y=c_{k})=\frac{\sum_{i=1}^{N}I(x_{i}^{(j)}=a_{jl},y_{i}=c_{k})+\lambda}{\sum_{i=1}^{N}I(y_{i}=c_{k})+S_{j}\lambda}$$

注意到这里分母都同时增加了相应倍数的$\lambda$，这是为了是为了让先验概率以及条件概率的和都等于1。

## 小总结
所有概率计算方法都有了，那么针对不同数据，我们只需要找出可以使某个输入x的相关后验概率最大化的输出即可得到答案咯~~







