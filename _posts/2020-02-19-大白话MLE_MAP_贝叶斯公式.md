---
layout:     post
title:      大白话MLE，MAP，贝叶斯公式
subtitle:   统计基础很重要啊
date:       2020-02-16
author:     Noah-Yiliu
header-img: img/taitou.jpg
catalog: true
tags:
    - Statistics
---

# 大白话MLE，MAP，贝叶斯公式
## 写在前面
本想今天更新一下**朴素贝叶斯**算法，但是前提是我们已经理解作为必备概念的**MLE（极大似然估计）**，**MAP（最大后验概率估计）**以及**贝叶斯公式**。这些概念也经常出现在面试题中，记得上次面试dsfs的实习生，一个极大似然估计的问题就把我难倒了，这也一棒子打醒了我，哪里不会补哪里！因此我恶补了一下这方面的知识，今天就带大家做一个简单的复习（其实是我自己想复习）。

## 贝叶斯公式
为什么要先说贝叶斯公式呢，因为这个公式实际上**“囊括”**了MLE以及MAP这两个概念。我们先来看一下，贝叶斯公式的写法：$$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$
这个公式到底啥意思呢？我们假设事件**A**代表的是**猕猴桃的皮是青色**的，事件**B**代表**猕猴桃不好吃**。我们先看等式右边，分子中的$P(B|A)$就代表如果已知猕猴桃他的皮是青色的，那么他不好吃的概率有多大。$P(A)$则代表猕猴桃是青色这件事发生的概率。分母中的$P(B)$代表的是猕猴桃不好吃这件事发生的概率。

那么这个公式想要求解的值，也就是等式左边的值是什么呢？可以理解为，当已知猕猴桃不好吃这个前提，是因为他皮色是青色的概率（因为可能这种猕猴桃不成熟）。也就是说，这个猕猴桃不是因为皮色为青色，不是别的原因。那么这个别的原因包含在哪里呢？我们把$P(B)$拆开的话可以写成如下形式：$$P(B)=P(B|A)P(A)+P(B|非A)P(非A)$$这里**$非A$**代表的就是除了A以外的原因。

为什么这个公式这么重要呢？我们接着引入MLE以及MAP你就能发现这个公式的重要性了。

## MLE（最大似然估计）
### 似然函数
首先介绍一下似然函数。其实现在你可以轻松理解似然函数了，就是之前提到的$P(B|A)$。不过这里我们把事件A用$\theta$代替并命名为参数，把事件B用$x_{0}$代替作为随机变量。那么，当$\theta$已知的时候，这个公式就变成了求关于随机变量的一个**概率函数**；当$x_{0}$已知呢？亦柳告诉您，这就是**似然函数**。

![Markdown](http://i2.tiimg.com/708998/68217126293ef31e.jpg)

似然函数怎么求呢，还是搬出生动的🌰来理解它！如前述，当参数未知，随机变量已知，$P(x_{0}|\theta)$这个公式变成了似然函数的公式。这里随机变量一定是符合某个分布的，那我们假设这个分布是二项分布吧（二项分布也不知道的我要敲脑袋了），例如你随机从一个框子里抽出十次猕猴桃，抽出的结果$x_{0}$是“好好好好坏坏坏坏好好”，而我们想求的模型参数$\theta$是抽出好的概率，那么$P(x_{0}|\theta)$就可以写成：$$P(x_{0}|\theta)=\theta^{4}(1-\theta)^{4}\theta^{2}$$

### 极大似然估计
那么既然我们都有似然函数了，什么叫最大似然估计呢？非常直白！**求这个函数的最大值，这时对应的参数值就是最大似然估计值**。那你要问我上面的函数最大值怎么求，我看前人的gcb是盖不住了。通过求解以上函数的一阶导数，使他为零得到$\theta$的估计值，再通过二阶导小于零确定此处$\theta$对应的这个极值是极大值而不是极小值即可得到答案。

这里有一点需要注意，上述分布属于非常简单的二项分布，最终的到的似然函数即便是直接求导耶是可以计算的。但是当似然函数很复杂的时候，作为连乘形式的求导就很难了，这时候我们一般会对似然函数进行对数化处理，也称**对数似然函数**，神奇的事情发生了，连乘概率变成了连加，那计算起来就会更加方便了。这里我们如果针对以上案例进行计算的话，求出的正面向上概率的最大似然估计值为**0.6**，也就是在这个概率下，发生“好好好好坏坏坏坏好好”是***最有可能的***。

提问！做对数化以后不会影响似然函数的相对大小吗？不会，因为对数函数是单调递增函数喔。

![Markdown](http://i1.fuimg.com/708998/2238ae7f27853672.jpg)

##MAP（最大后验估计）
我们已经学习完了最大似然估计了，那么什么是最大后验估计呢？我们知道最大似然估计中，我们需要最大化的概率是$P(x_{0}|\theta)$，而最大后验估计需要最大化**$P(x_{0}|\theta)P(\theta)$**，请大家回忆一下贝叶斯公式（往上翻翻）中右式的分母，是不是一毛一样。没错，这里多出来的这一项，我们称为**先验概率**。为啥叫先验概率呢？也就是日常生活中我们的一些先验知识，比如硬币如果两面是均匀的，那么我抛出正面和反面的概率应该是0.5，这种认知就是先验。回归到我们的公式中，由于我们不能确定一定能出现某个参数，因此$P(\theta)$也不会总是1。贝叶斯学派因此认为，最大化似然函数并不是狠**靠谱**，还需要加上这一项。

嗯，仿佛很有道理，那么为什么分母给丢了呢，贝叶斯公式不是还有个分母吗。我们看到这里分母就是$P(x_{0})$，也就是这个随机时间出现的概率。这是个已知量，是通过我们已有的数据算出来的，当然不用考虑咯。

## 写在最后
大概就这么多了，有了这些知识，我们就可以进入**朴素贝叶斯**了~~明天见各位。











