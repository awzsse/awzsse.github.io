---
layout:     post
title:      大白话支持向量机（核函数，SMO待补充）
subtitle:   难嚼的骨头
date:       2020-03-03
author:     Noah-Yiliu
header-img: img/taitou.jpg
catalog: true
tags:
    - Machine Learning
---
# 大白话支持向量机（核函数，SMO待补充）
## 前言 or 近期总结
眨眼一个多月过去了，自己的小博客也在缓慢成长着。原本计划一个月的时间把李航老师的《统计学习方法》大致过一遍顺便写完各个算法的博客，现在看来这个flag是要立不住了。不过即便如此我还是认认真真的学习了本书至少一半的内容，在今天完成支持向量机的博客之后，我相信我已经算半只脚踏进机器学习领域了，再也不是一个什么都不懂的小白了~✿✿ヽ(°▽°)ノ✿。在这一个月内我最大的感触是在技术领域如果想要求发展不能简简单单的“学过”，一定要有相应的产出。不论是学前端的同学做出一个好看的网页，还是学数据库的同学做出一个完整的数据库项目，亦或者像我这样在新领域刚刚起步的人完成了一定量的技术博客等。

这里定下一个阶段性小目标，3月份完成本书绝大部分的算法博客，四月份可能就要忙于春招和秋招的准备了，希望一切的努力都不会白费~

## 支持向量机（SVM）难在哪里
现在回头再看支持向量机，已经不再会被他晦涩的公式和拗口的表达吓到了。但是回想刚刚翻开第七章的时候内心还是有一丢丢害怕，是什么算法要李老师花40多页去叙述，毕竟选择李老师的书就是因为他的语言既简洁又容易理解哇。总结起来，主要是由于这个算法涉及到了很多**优化问题**的概念和解法，同时它所提及的**核函数**以及**SMO**算法可能需要慢慢消化。当然，我们不能一口吃成个胖子，一切的一切还要从它最简单的模型说起。

## 线性可分支持向量机&硬间隔最大化
当谈到分类器的时候，我们的猕猴桃君🥝就会出现了。还记得在博客创始之初提到的[感知机模型](https://awzsse.github.io/2020/01/22/%E5%A4%A7%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95-%E6%84%9F%E7%9F%A5%E6%9C%BA/)吗？我们希望对一群猕猴桃分类，仅仅分成两类，我们创造了感知机这个二分类器。其实**线性可分支持向量机**希望解决的问题与感知机类似，由于前提假设为数据的线性可分，因此我们也可以画出一个超平面将数据完全分开。那么不同点在哪呢？回忆一下，感知机是通过**误分类最小**的策略，这样画出来的平面是不是可以有无数个？（a,b,c均可）

![](https://github.com/awzsse/awzsse.github.io/blob/master/img/svm_hp.png?raw=true)

而SVM则是通过**间隔最大化**这个准则去求解一个最优平面，这里就是选取$b$作为最优的超平面（是不是看起来离两边的最近的🥝距离最大？），并且可以证明这个最大间隔分离超平面是一定**存在且唯一**的，具体参考[这个哥们](https://www.cnblogs.com/lizhizhuang/p/8613233.html)的证明过程，非常详细，并且字迹清晰，真羡慕。下面给出具体的定义。

> 给定线性可分数据集，通过间隔最大化或者等价求解凸二次规划问题得到的分离超平面为：
> 
> $$w^{*}\cdot x+b^{*}=0$$
> 
> 以及相应的决策函数：
> 
> $$f(x)=sign(w^{*}\cdot x+b^{*})$$
> 
> 称为线性可分支持向量机。

那么什么是间隔呢，还有什么是硬间隔，我们总要有个计算方法吧。别急，我们首先来看两个概念，即**函数间隔**以及**几何间隔**。

### 函数间隔和几何间隔
假设我们有一个数据集$T$以及一个超平面$w\cdot x+b=0$，我们将数据点$(x_{i},y_{i})$关于这个**超平面**的**函数间隔**定义为：

$$\bar{\gamma}_{i}=y_{i}(w\cdot x_{i}+b)$$

而定义**数据集**关于超平面的**函数间隔**定义为：

$$\bar\gamma=\min_{i=1,...,N}\bar\gamma_{i}$$

也就是说**数据集**关于这个超平面的函数间隔是所有**数据点**关于这个平面函数间隔的**最小值**。我直接用这个间隔来决定呗？看看能不能找到两个点，他们分别是正负例中距离超平面最近的点，直到找到最大函数间隔的两个点不就完了么。道理是这么个道理，但是考虑到函数间隔的计算方法，如果我们成比例的增加$w$和$b$，我的函数间隔是变大的，但是超平面确没变化啊~这是个问题，我们怎么解决呢？通过引入几何间隔，其定义如下：

给定训练数据集$T$以及一个超平面$w\cdot x+b=0$，我们将数据点$(x_{i},y_{i})$关于这个**超平面**的**几何间隔**定义为：

$$\gamma_{i}=y_{i}(\frac{w}{||w||}\cdot x_{i}+\frac{b}{||w||})$$

而定义**数据集**关于超平面的**几何间隔**定义为：

$$\gamma=\min_{i=1,...,N}\gamma_{i}$$

这时，我们对超平面的法向量做出了一些限制，如规范化，这样就让函数间隔变成了几何间隔。考虑这个定义式，如果我们的点被正确分类了，那么几何间隔就是正的，如果错误分类那么就是负的，同时不论你的超平面参数怎么成比例的变化，几何间隔都不会变。这是我们需要用到的参数，贼棒。

### 间隔最大化
既然已经知道了需要求解的间隔，那么我们极大化这个间隔得到的平面就可以以足够大的**确信度**将数据分开~也就是说对于最难分开的实例点，我们都有足够的信心把他们分开。因此我们得到了一个初始的约束最优化问题：

$$\max_{w,b} \space\gamma$$

$$s.t. \space y_{i}(\frac{w}{||w||}\cdot x_{i}+\frac{b}{||w||})\geq \gamma, i=1,2,...,N$$

警告⚠️，从这里开始我们将面对SVM的第一个难关，也就是**凸优化问题**的求解，我会尽量用最简单的语言去解释，当然，配合李航老师的《统计学习方法》食用味道更佳。

我们首先看看上面列出的**带约束条件**的最大化问题。考虑到几何间隔和函数间隔的关系，我们改写上述问题为：

$$\max_{w,b} \space\frac{\bar\gamma}{||w||}$$

$$s.t. \space y_{i}(w\cdot x_{i}+b)\geq \bar\gamma, i=1,2,...,N$$

为什么这里又要算回函数间隔了？这可不是无用功，我们知道函数间隔的取值并不影响这个最优化问题的解，成比例增加$w$和$b$的时候，函数间隔也会成比例增加，这并不影响上述限制条件，因此我们可以简化一下，取$\bar\gamma=1$，同时将最大化问题变成最小化问题，那么我们就得到了最终的优化问题，注意这是个**凸二次规划**问题，是比较容易求解的形式，这也是我们费尽周折转换其形式的原因：

$$\min_{w,b} \frac{1}{2}||w||^{2}$$

$$s.t. \space y_{i}(w\cdot x_{i}+b)-1\geq 0, i=1,2,...,N$$

关于**凸二次规划**的定义，简单说来，如果目标函数（二次函数）和**不等式**约束函数都是$R^{n}$上的连续可微凸函数，约束函数都是仿射函数的时候，称为“凸二次规划”。因此求解出上述优化问题，得到最终的分离超平面以及分类决策函数，问题得解。我们先不说求解方式，先提一个很有意思的概念叫做**支持向量**。

> 训练数据集的样本点和分离超平面距离最近的样本点实例称为支持向量（support vector）

定义很短，但是支持向量的意义非凡。我们可以看出，支持向量就是使得不等于约束等于零的点！！因此，正例和负例点由于$y_{i}$不一样，分别分布在两个平面H1，H2上，这两个平面之间的距离就称为**间隔**，而H1，H2称为间隔边界。如图所示：

![](https://github.com/awzsse/awzsse.github.io/blob/master/img/svm_margin.png?raw=true)

我们发现，对分离超平面的决定起到关键作用的只有这些支持向量，在间隔边界外移动别的点对整个超平面并没有影响。

### 对偶算法求解线性可分支持向量机的最优化问题
⚠️⚠️⚠️

重要的事情说三遍，一下涉及到[拉格朗日函数](https://www.zhihu.com/question/38586401)以及[对偶问题](https://www.zhihu.com/question/58584814)等我会用通俗的语言去解释，保证大家可以充分理解其在SVM上的应用，但是如果想深入了解请大家参考给出的链接，这真的不是三言两语可以说清楚的。

三言两语第一语，首先说一下什么是**拉格朗日函数**。我们已经拥有了以上带有约束条件的最优化问题，可是我们该怎么入手呢，毕竟要同时考虑两个问题是不容易的。这时候我们就要通过构建**拉格朗日函数**，利用某个**乘子**把约束条件给他归纳到原本我们想要求解的优化问题中，那不就可以既解决优化问题，又符合了约束嘛~~我们用$f(x)$表示我们不带约束条件的优化问题，$g(x)$作为我们的限制条件，考虑到我们的约束条件是XXX$\geq 0$，因此我们将整个拉格朗日函数构建成如下形式：

$$L=f(x)-\alpha g(x)$$

$\alpha$也称为**拉格朗日乘子**，它是一个大于等于零的数，如果约束条件满足的话，他和约束条件的乘积肯定是小于等于零的，加个负号就是大于等于零的（绕的过来吧），所以针对这个问题，我们需要做的是在**最大化**关于$\alpha$的这一项基础上**最小化**原始的优化函数。为什么这么说，**最大化**后面一项的意思是我们要尽量误分类点减少，直到完全约束条件。因此整个问题可以表示为：

$$\min_{w,b}\max_{\alpha}L$$

也就是**原始问题**，这是一个极小极大问题。将我们之前整理的信息带入，则在线性可分支持向量机的学习中，拉格朗日函数以及整个极小极大问题可以表示为：

$$L(w,b,\alpha)=\frac{1}{2}||w||^{2}-\sum_{i=1}^{N}\alpha_{i}y_{i}(w\cdot x_{i}+b)+\sum_{i=1}^{N}\alpha_{i}$$

$$\min_{w,b}\max_{\alpha}L(w,b,\alpha)$$

![](https://github.com/awzsse/awzsse.github.io/blob/master/img/lagrange.jpg?raw=true)

从上面这个图我们可以分析出，初始优化问题的某个等高线和约束条件相切的位置就是我们要求解的点，如果约束条件是不等式的话，图中的约束条件就不是一条线了，而是某个区域。

咳咳，三言两语第二语。我们下面的目标就是求解这个极小极大了。那么问题又来了，如果我们通过先求解极大（离式子近的那个）问题，也就是对$\alpha_{i}$求偏导使其等于零，得到的公式是什么呢？大家简单计算一下就会发现，我们消除的是$\alpha_{i}$，因此得到的肯定是关于$w,b$等的结果，这样我再去求解极小问题就会显得很麻烦。所以我们引入了**拉格朗日对偶问题**，根据定义，原问题的对偶问题就是一个极大极小问题，也就是：

$$\max_{\alpha}\min_{w,b}L(w,b,\alpha)$$

我们通过求解这个对偶问题就可以得到原始问题的答案，我们可以看到，如果先解决极小问题就可以把$w$和$b$统统利用$\alpha_{i}$来表示，再解决极大问题就会比较容易（只有一个参数了）。

重点来了，这个对偶问题要满足什么条件才可以得到和原始问题一样的答案呢？下面给出定理：

> 首先假设原优化函数$f(x)$和不等式约束$g_{i}(x)$是凸函数，等式约束$h_{j}(x)$是放射函数（这里暂时没有等式约束），切不等式约束严格可行，那么存在$x^{\*},\alpha^{\*},\beta^{\*}$分别是原始问题和对偶问题的解。同时，这些参数满足下面的**KKT**条件：
> 
> $$\nabla_{x}L(x^{*},\alpha^{*},\beta^{*})=0$$
> 
> $$\alpha_{i}^{*}g_{i}(x^{*})=0，该条件又称为松弛互补条件$$
> 
> $$g_{i}(x^{*})\leq 0$$
> 
> $$\alpha_{i}^{*}\geq 0$$
> 
> $$h_{j}(x)=0$$

因此，我们只需要解出对偶问题就能直接得到原始问题的解。下面总结一下对偶算法的过程。

（1）将拉格朗日函数先针对$w$和$b$求偏导，过程省略大家自己推，可以得到的结论为：

$$w=\sum_{i=1}^{N}\alpha_{i}y_{i}x_{i}$$

$$\sum_{i=1}^{N}\alpha_{i}y_{i}=0$$

（2）将其带入原始的拉格朗日方程后，再针对$\alpha$求极大（或者求复值极小，这里用极小）：

$$\min_{\alpha}\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}\cdot x_{j})-\sum_{i=1}^{N}\alpha_{i}$$

$$\sum_{i=1}^{N}\alpha_{i}y_{i}=0，\alpha_{i}\geq 0$$

（3）具体如何求解上述的$\alpha$需要通过著名的SMO算法来完成，我们首先假设已经得到了一组$\alpha_{i}^{*}$，那么根据之前求偏导的结果，我们可以得到$w$的值为：

$$w^{*}=\sum_{i=1}^{N}\alpha_{i}^{*}y_{i}x_{i}$$

那么如何求得$b$的值呢？由于这几个参数是满足KKT条件的，记得我们提到的一个的**松弛互补条件**吗？这里由于：

$$\alpha_{i}^{*}(y_{i}(w^{*}\cdot x_{i}+b^{*})-1)=0$$

因此对于$\alpha_{i}^{\*}>0$的点，一定使得不等式约束为零，那么我们选择某个$\alpha_{j}^{\*}>0$，可以求得：

$$b^{*}=y_{j}-\sum_{i=1}^{N}\alpha_{i}^{*}y_{i}(x_{i}\cdot x_{j})$$

从而得到分类决策平面：

$$f(x)=sign(\sum_{i=1}^{N}\alpha_{i}^{*}y_{i}(x\cdot x_{i})+b^{*})$$

#### 再提支持向量
还记得我们之前说的支持向量吗，这里我们利用对偶形式在给出它的一个定义。由KKT互补条件可知，当$\alpha_{i}^{\*}>0$的时候，不等式约束取零，意味着对应于$\alpha_{i}^{\*}>0$的实例$x_{i}$一定在边界上也就是支持向量。因此，支持向量就是那些对应$\alpha_{i}^{\*}>0$的$x_{i}$。

## 线性支持向量机和软间隔最大化
总结完了线性可分支持向量机，怎么将其扩展为线性不可分的问题呢？假设给定了一个数据集$T$，我们发现除了一些特异点（outlier）以外，大部分的点都是线性可分的。也就是说，我们要想出一个规则可以**包容**这些点。那么针对每个样本点，我们引入一个松弛变量$\xi_{i}\geq 0$使得约束条件变成：

$$y_{i}(w\cdot x_{i}+b)\geq 1-\xi_{i}$$

同时，我们针对每个松弛变量都让其支付一个代价，则修改后的**原始问题**变成了：

$$\min_{w,b} \frac{1}{2}||w||^{2}+C\sum_{i=1}^{N}\xi_{i}$$

$$s.t.\space y_{i}(w\cdot x_{i}+b)\geq 1-\xi_{i}$$

$$\xi_{i}\geq 0，i=1,2,...,N$$

在调和参数C给定的情况下，可以证明这个凸二次规划问题有唯一的$w$解，而$b$却不一定，存在于一个区间。这称为**软间隔最大化**，该算法与前面的线性可分支持向量机一起并称为**线性支持向量机**。

### 对偶算法
有了前面的基础，这里我们可以直接给出**线性支持向量机**的对偶算法。这里为什么我们可以将两者合并，我们可以发现，如果当所有的$\xi_{i}$都等于零的时候，那么就回归了**线性可分支持向量机**，因此该式具有普适性。因此，线性支持向量机的学习算法为：

（1）构建拉个朗日函数：

$$L(w,b,\xi,\alpha,\mu)=\frac{1}{2}||w||^{2}+C\sum_{i=1}^{N}\xi_{i}-\sum_{i=1}^{N}\alpha_{i}(y_{i}(w\cdot x_{i}+b)-1+\xi_{i})-\sum_{i=1}^{N}\mu_{i}\xi_{i}$$

（2）对偶问题是拉格朗日函数的极大极小问题，因此将拉格朗日函数对三个参数求偏导，得到：

$$w=\sum_{i=1}^{N}\alpha_{i}y_{i}x_{i}$$

$$\sum_{i=1}^{N}\alpha_{i}y_{i}=0$$

$$C-\alpha_{i}-\mu_{i}=0$$

其中第三个条件结合$\mu_{i}\geq 0$整理为$0\leq \alpha_{i}\leq C$。这些参数带入原始的拉格朗日函数可以得到：

$$\min_{\alpha}\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}\cdot x_{j})-\sum_{i=1}^{N}\alpha_{i}$$

$$\sum_{i=1}^{N}\alpha_{i}y_{i}=0，\alpha_{i}\geq 0$$

$$0\leq \alpha_{i}\leq C$$

（3）同理，求解出一组$\alpha_{i}^{*}$后，我们可以得到：

$$w^{*}=\sum_{i=1}^{N}\alpha_{i}^{*}y_{i}x_{i}$$

那么$b$的值怎么决定呢？这里，我们根据KKT条件，同样可以得到松弛互补条件，这里由于多出一个参数$\mu$，因此：

$$\alpha_{i}^{*}(y_{i}(w^{*}\cdot x_{i}+b^{*})-1+\xi_{i})=0$$

$$\mu_{i}^{*}\xi_{i}^{*}=0$$

类似前面给出的分析，如果说$\alpha_{i}^{\*}>0$，我们可以通过不等式约束为零求出$b^\*$。但是注意这里我们的$\alpha_{i}$还要小于等于$C$。考虑如果等于$C$的时候，根据之前求偏导的结果，$C-\alpha_{i}^\*-\mu_{i}^\*=0$，所以$\mu_{i}^\*=0$，根据第二个松弛互补条件，这时$\xi_{i}^\*$不能为零。因此，如果想要求出$b^\*$，我们必须在$0<\alpha_{i}^\*<C$中选择一个$\alpha_{i}^\*$，从而根据不等式约束求出$b^{\*}$。

（4）从而得到分类决策平面：

$$f(x)=sign(\sum_{i=1}^{N}\alpha_{i}^{*}y_{i}(x\cdot x_{i})+b^{*})$$

### 再 再 再提支持向量
软间隔的支持向量要稍微复杂一点。由于他的取值范围是0到$C$，因此分为以下几种情况。

* 当$\alpha_{i}<C$时，$\xi_{i}=0$，这时支持向量在间隔边界上。
* 当$\alpha_{i}=C$，$0<\xi_{i}<1$时，分类正确，支持向量在间隔边界和超平面之间。
* 当$\alpha_{i}=C$，$\xi_{i}=1$时，支持向量在超平面上。
* 当$\alpha_{i}=C$，$\xi_{i}>1$时，支持向量在超平面误分类一侧。

### 合页损失函数
说实话，我已经快把手敲断了。终于来到了线性支持向量机的最后一个关键概念，合页损失函数！我们知道，前面所有的优化问题来源于**间隔最大化**这个概念，那么我们如果从损失函数的角度来想呢，因此考虑以下目标函数的最小化：

$$\sum_{i=1}^{N}[1-y_{i}(w\cdot x_{i}+b)]_{+}+\lambda||w||^2$$

这个式子的第一项称为**经验损失函数**也称**合页损失函数**，衡量这个模型拟合程度的好坏，第二项称为**结构损失函数**，也就是整个结构是不是过于复杂。实际上，这个损失函数与之前的原始问题是等价的。正好上次作业证明过，现在附上截图：

![](https://github.com/awzsse/awzsse.github.io/blob/master/img/hingeloss.png?raw=true)

## 写在后面
本篇博客暂时只提及了线性支持向量机的全部内容，关于核函数以及SMO还需后续补充。




