---
layout:     post
title:      大白话EM算法
subtitle:   当你不知道中途发生了什么，你就要当心了
date:       2020-03-15
author:     Noah-Yiliu
header-img: img/taitou.jpg
catalog: true
tags:
    - Machine Learning
---
# 大白话EM算法
## 前言
这篇博客的内容基本上是来自李航老师的《统计机器学习》第九章。同时我仅仅叙述了**EM算法**以及其在**GMM模型**上的应用，关于其推广部分我还没有做细致的了解。文章结构与李航老师在顺序上稍有不同，并在特殊的地方做出了自己的解释。不得不服的是李航老师真的是用最简洁的语言叙述了这个算法的核心内容，同时我还参考了李义达老师的EM算法讲解视频，这也让我对这个算法的理解更加深刻了，有兴趣的同学也可以去了解一下相关的课程。


## 高斯混合模型（GMM）
由于EM算法的一个很重要的运用就是计算**高斯混合模型**，那么我们首先先对高斯混合模型做一些总结。什么是高斯混合模型呢？高斯模型大家一定很熟悉了，简而言之就是一群遵循**正态分布**的随机变量组成的概率分布模型，其参数一般包括均值$\mu$以及方差$\sigma^2$。好，现在假设我们有两组随机变量如下图：

![](https://github.com/awzsse/awzsse.github.io/blob/master/img/GMM.png?raw=true) 

左边的$X_1$符合$G_1$这个分布，右边的$X_2$符合$G_2$这个分布。显然这是两个均值不同的高斯模型。那么假设$X_1，X_2$是来自同一批随机变量，我们怎么去统一的表示这个新的分布模式呢？这其实就是高斯混合模型，通过几个简单高斯模型的叠加就可以得到，图中的GMM就表示了他的**概率密度函数**。

我们给出如下的定义：

> 高斯混合模型指的是具有以下形式的概率分布模型:
> $$P(y|\theta)=\sum_{k=1}^{K}\alpha_{k}\phi(y|\theta_{k})$$
> 其中$\alpha_{k}$是系数，同时满足$\sum_{k=1}^{K}\alpha_{k}=1$，而后一项是高斯分布密度函数，其中第$k$个分模型的密度函数为：
> $$\phi(y|\theta_{k})=\frac{1}{\sqrt{2\pi\sigma_{k}}}exp(-\frac{(y-\mu_{k})^2}{2\sigma_{k}^2})$$

这里的系数$\alpha_{k}$表示了每个分模型在决定概率分布上所占的比重。

## EM算法的引入
为什么我要先说高斯混合模型呢？其实李老师是先用了一个三硬币模型引入EM算法，后面才说道高斯混合模型的，但是就我个人学习过程来看，一开始其实我有点被这个例子弄晕了。所以我选择直接用高斯混合模型来进行EM算法的引入可能更白话一些。那么什么是EM算法呢？我们知道，如果确定我们的数据是符合某个分布的，比如某个单个的高斯分布，那么我们怎么预测他的参数呢？第一个引入脑海的就是是**极大似然估计**。

通常来说，我们只要能列出某个似然函数，或者对数似然函数，然后对其进行极大化，就可以直接得到关于**观测值**的**最可能的参数**。好了，核心内容出现了，我们这里的**观测值**是现实可以观测的。有同学说这不是废话么，不然还叫什么观测值。说的不错，学术上我们将这种变量称为**观测变量（observable variable）**。但是我概率模型有时候还会含有另一种无法观测的变量，也称**隐变量**或者**潜在变量（latent variable）**。在高斯混合模型中，我的某个数据属于的某个分布，这就是隐变量。试着理解一下，我们通过现有的观测数据去估计参数，但是如果是高斯混合模型这种模型，我们**中间**还夹杂了一步，那就是估计哪个**分模型**的参数，这就是EM算法要解决的问题。

## EM算法的推导
话不多说，我们以GMM为模型一步一步来推导一下EM算法。首先我们针对一个含有隐变量的概率模型，我们的目标仍然是极大化观测数据关于参数的似然函数，即：

$$L(\theta)=\log{P(Y|\theta)}=\log{\sum_{z}P(Y,Z|\theta)}=\log{\sum_{z}P(Y|Z,\theta)P(Z|\theta)}$$

解释一下，这里的$Z$实际上就是我们说的隐变量，我们极大化对于观测数据的似然函数是最终目标，但是由于隐变量的存在，我们需要考虑在不同的子模型中，也就是不同的单个高斯分布下与观测值的联合概率分布。同时联合概率分布可以拆成两个条件概率的乘积。

但是我们发现，由于隐变量的存在，同时式中还有求和，即积分的存在，会导致整个极大化变得非常困难。EM提出了一种办法，那就是我们不要一步到位，我们需要做的是**每次进步一点**。怎么理解呢，我们假设在第$i$次迭代以后得到了一个参数估计值为$\theta^{(i)}$（注意如果是高斯混合模型的话，这里的参数就是$[\mu_1^i,\mu_2^i,...,\mu_k^i,\sigma_1^i,\sigma_2^i,...,\sigma_k^i,\alpha_1^i,\alpha_2^i,...,\alpha_k^i]$）。这样一来我们肯定希望新得到的这个值$\theta$可以让$L(\theta)$增加，也就是$L(\theta)>L(\theta^{(i)})$。

我们针对$L(\theta)$做出一个变形：

$$L(\theta)=\log{(\sum_{Z}P(Z|Y,\theta^{(i)})\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})})}$$

为什么要做这一步呢？我们为了构造一种**期望的函数**形式，这样可以利用**Jensen不等式**找到$L(\theta)$的下界。先别晕我们来给大家普及一下什么是**Jensen不等式**，然后再说我们找到这个下界用来做什么。

### Jensen不等式
我们假设某个函数是凸函数，他一定符合一个性质那就是，期望的函数大于函数的期望。什么意思呢，我们看看下面这个图：

![](https://github.com/awzsse/awzsse.github.io/blob/master/img/Jensen.png?raw=true) 

这个图表示一个凸函数，我们给定一个$p$作为取$x_1$的概率，那么$1-p$就是取$x_2$概率。好那么我们**Jensen不等式**说的就是关于这个变量期望的函数一定大于等于关于这两个变量的函数的期望。也就是图中橙色点比粉红色的点要高一点。好了简单了解Jensen不等式我们就继续往后看。

### 对数似然函数的下界
我们观察之前提到的对数似然函数的变形，可以发现，我们添加的部分

$$\sum_{Z}P(Z|Y,\theta^{(i)})$$

其实可以看做我们的$p$，也就是说我们对后面那一堆东西做了一个期望，再放到$log$函数，那这不就是**期望的函数**吗？那我们一定可以列出下面这个式子：

$$原式\geq \sum_{Z}P(Z|Y,\theta^{(i)})\log{\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}}$$

我们给上面这个式子命名一下称为：

$$B(\theta,\theta^{(i)})=\sum_{Z}P(Z|Y,\theta^{(i)})\log{\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}}$$

那么这个$B(\theta,\theta^{(i)})$就是我们需要求解的下界了。现在回归到第二个问题，求这个下界有什么用呢？如果这个函数是我们目标函数的下界的话，只要我们极大化这个函数，是不是等于近似的极大化了$L(\theta)$？因此，我们只需要求解一个参数使得：

$$\theta^{(i+1)}=arg\max_{\theta}B(\theta,\theta^{(i)})$$

$$\theta^{(i+1)}=arg\max_{\theta}(\sum_{Z}P(Z|Y,\theta^{(i)})\log{\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}})$$

由于$\theta^{(i)}$相关的项是我们已经求解出来的，可以看做常数，因此$log$函数中的分母可以去掉，最终得到如下式子：

$$\theta^{(i+1)}=arg\max_{\theta}(\sum_{Z}P(Z|Y,\theta^{(i)})\log{P(Y|Z,\theta)P(Z|\theta)})$$


$$\theta^{(i+1)}=arg\max_{\theta}(\sum_{Z}P(Z|Y,\theta^{(i)})\log{P(Y,Z|\theta)})$$

这里我们得到了EM算法的核心函数，叫做**Q函数**：

$$Q(\theta,\theta^{(i)})=\sum_{Z}P(Z|Y,\theta^{(i)})\log{P(Y,Z|\theta)}$$

因此总结起来，EM算法就是通过对一次一次的迭代来极大化下一次的**Q函数**，从而保证**对数似然函数**不断增加。这里可以证明该算法形成的对数似然函数序列是递增的，同时会在某个值收敛（不做证明），但是需要注意的一点是，最终得到的收敛的对数似然函数对应的参数，不一定是全局最优的，也就是说该算法可能陷入局部最优，这时候选择一个比较好的初始值就显得尤为重要。

## EM+GMM，强强联合
我们已经具有了所有的储备知识，现在介绍EM算法最有名的应用之一，那就是高斯混合模型的求解。同时我们需要解释**EM**算法的**Expectation**以及**Maximization**的意义。

（1）我们首先要明确隐变量的，我们定义出初始的**似然函数**以及**对数似然函数**。

![](https://github.com/awzsse/awzsse.github.io/blob/master/img/GMM_EM_1.png?raw=true)

（2）其次，我们进入到EM算法的E步，这一步非常巧妙的将隐变量通过期望的形式转换成了我们已知的参数，同时得到一个关键的概念--观测数据的响应度，用于其余参数的计算。

![](https://github.com/awzsse/awzsse.github.io/blob/master/img/GMM_EM_2.png?raw=true)

（3）我们通过M步，求出Q函数极大值对应的参数值，也就是$i+1$步的参数值。

![](https://github.com/awzsse/awzsse.github.io/blob/master/img/GMM_EM_3.png?raw=true)

（4）通过不断迭代算法，直到对数似然函数数值不再有明显的变化为止得到最终参数估计。


## 总结
针对EM算法以及其在GMM模型的实际应用基本就是这样，是不是觉得干货满满？当然，EM算法的用处远不止于此，还有很多需要我们继续探究。