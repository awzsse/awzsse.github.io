---
layout:     post
title:      大白话逻辑斯谛回归
subtitle:   面试必考之一
date:       2020-02-21
author:     Noah-Yiliu
header-img: img/taitou.jpg
catalog: true
tags:
    - Machine Learning
---

# 大白话逻辑斯谛回归

## 写在前面
经过一下午的复习和“剽窃”，我也算弄懂了逻辑回归的一些主要内容，作为面试常考题，我们对逻辑斯谛回归的**回归模型建立**以及**参数估计方法**做出了较为详细的解释，以助大家更加轻松的理解。

另外，这篇涉及到的部分公式推导我是用手写的形式呈现的，希望大家谅解。

## 逻辑斯谛回归的“逻辑”在哪里
### 逻辑斯谛模型
网上针对逻辑斯谛回归的帖子非常非常多，包括李航老师针对逻辑斯谛原理的叙述也非常完备。但我发现在我看书的时候还是没有理解为什么要用这个具体的逻辑斯谛分布作为模型基础，他到底好在哪里？我们首先来介绍一个概念，**逻辑斯谛分布**。

> 逻辑斯谛分布：指的是连续变量X服从以下分布函数的分布。
> $$F(X)=P(X\leq x)=\frac{1}{1+e^{-(x-\mu)/\gamma}}$$

其中$\mu$代表了他的位置参数，而$\gamma>0$代表了他的形状参数。我们看到当$\mu=0$的时候他的形状如下图所示，是一条S形的曲线。（来自西瓜书）

![lrgraph](https://github.com/awzsse/awzsse.github.io/blob/master/img/logistic.jpg?raw=true)

也就是说，这条线关于点$（\mu,\frac{1}{2}）$呈中心对称。那么聪明的同学应该已经知道我要说是什么了。对，**逻辑斯谛回归模型**就是依据逻辑斯谛分布建立的，我们先直接看一下模型是什么样的，再对其合理性进行解释。这里回归模型为**二项回归模型**，解决二分类问题。

> 二项逻辑斯谛回归模型是一个**条件概率分布**，具体的分布方式如下：
> $$P(Y=1|x)=\frac{exp(w·x+b)}{1+exp(w·x+b)}$$
> $$P(Y=0|x)=\frac{1}{1+exp(w·x+b)}$$

他的分类方法就是针对不同的输入分别计算这两个条件概率分布，根据值的大小判断输出类别。

也就是说，这个模型中$Y$的输出只有0和1，其中$w$和$b$是参数，称为**权值向量**以及**偏置**。大部分时候为了简化公式，我们会把$b$写进$w$中（后面公式都是这个写法），因为$w$是一个**n维向量**，而$b$是一个标量，因此可以将$b$作为一个维度写进$w$，只需要同时对输入$x$这个随机变量相应维度添加一个1即可。也就是$w=(w^{(1)},...w^{(n)},b)$以及$x=(x^{(1)},...x^{(n)},1)$。

那么为什么要用逻辑斯谛这个模型呢，他好处是什么呢？

![why](https://github.com/awzsse/awzsse.github.io/blob/master/img/why.png?raw=true)

首先我们看到逻辑斯谛模型的**条件概率**的**取值范围**与逻辑斯谛分布一样都是0到1，同时两个条件概率的和恒为1。这对于概率来说非常合理。同时我们看到公式中$e$的指数部分是$w·x+b$，这不就是个线性回归么吗？（回归二字得到解释）他的取值范围是整个**实数集**，也就是说这个模型把实数集的输入映射到了0-1的输出，非常完美！

那么有同学要问了，那我为啥不映射一个别的函数，比如我就在0-1之间随便画一条曲线，通过一个映射将我的输入映射到这个曲线上，再它来做我的模型基础不行吗？我只能说，问出这个问题的同学都有认真思考过这个模型，但是存在即合理！我们现在就来考察这个模型的“合理性”。

### 模型“合理性”
我们先引入一个概念，就是一个事件发生的**几率**。
> 几率（odds）：一个事件发生概率和不发生概率的比值。也就是：
> 
> $$\frac{p}{1-p}$$
> 
> 而这个事件的对数几率或者logit函数为：
> 
> $$logit(p)=\frac{p}{1-p}$$

那么我们回到逻辑斯谛回归模型，这里我们将$p$代替为其中一个条件概率那么：

$$log\frac{P(Y=1|x)}{1-P(Y=1|x)}=w·x$$

是不是很神奇？也就是说针对这个**条件概率**的**对数几率**就是一个线性函数，那么对于实数范围内的输入，我们直接可以将其看成是“几率”，这就是别的函数所不拥有的性质。

## 模型参数估计
针对逻辑斯谛模型的学习问题，我们可以利用**极大似然法**估计其参数，具体推导如下。

![lr](https://github.com/awzsse/awzsse.github.io/blob/master/img/logistic%20regression.jpg?raw=true)

（原谅我粗糙的字迹）

其中**对数似然函数**相比于**似然函数**来说计算更加简单。在我们得到**对数似然函数$L(w)$**，那么可以让其值达到最大的参数$w$就是我们需要求解的答案（针对似然函数不太理解的同学可以参考我[之前的博客](https://awzsse.github.io/2020/02/16/%E5%A4%A7%E7%99%BD%E8%AF%9DMLE_MAP_%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F/)）。

因此问题就变成了针对对数似然函数的最优化问题。这里注意，有些地方使用的是**极小化损失函数**，本质上是一样的。这里运用**梯度上升**算法，通过计算该对数似然函数的**一阶导**（也称为**梯度**，图中的$△w$）作为权值更新的依据。每次针对前一次的参数$w$进行更新，更新的快慢通过$\alpha$决定。直到**梯度**小于某个阈值即可停止算法（想象一下，梯度上升的越来越慢，是不是就快达到顶点了，那么如果上升的程度也就是梯度小于某个阈值，相当于上升速度极其缓慢以至于可以近似看做最高点了）。同时，由于log函数是严格的递增函数，单调可微，因此对数似然函数一定可以找到极大值。

证明完成~~~

## 总结
在面试过程中，同学们千万要注意不要只简单的理解逻辑回归模型的写法，同时要注意其**模型的合理性**以及**模型参数估计**中**对数似然函数**的推导以及**梯度上升**的用法。








