---
layout:     post
title:      大白话多臂赌博机
subtitle:   不是赌博啊，我没有啊，别乱说啊三连
date:       2020-02-09
author:     Noah-Yiliu
header-img: img/taitou.jpg
catalog: true
tags:
    - Reinforcement Learning
---

# 大白话多臂赌博机
## 多臂赌博机问题的通俗理解
什么？**赌博机！**这跟数据分析个鬼的关系，这不就是碰运气么。小伙伴们可别被他的外表蒙蔽了，这可是**强化学习**的经典问题，也可以用来解决生活中的诸多问题。为什么呢？因为他是**多臂**的啊，哪吒不厉害吗？

![Markdown](http://i2.tiimg.com/708998/2629264af84d8d7b.jpg)

说来惭愧，作为品行兼优社会主义好青年的我也曾经在葡京，巴黎人“叱咤风云”（咳咳，至少赚回了酒店和路费）。但是我们今天要说的，可不是骰宝之流，而是大名鼎鼎的**多臂赌博机**问题，英文叫做***Multi Armed Bandit***。既然是大白话，我们就从最简单的角度来理解一下什么是**多臂赌博机**问题。

想必大家都知道老虎机长啥样吧，就算没玩过，那没吃过猪肉，还没见过猪跑么。喏，就是下面这个东西。

![Markdown](http://i1.fuimg.com/708998/4719084fffd891c4.jpg)

注意到红色的这个握把了没，每次当我们摇下握把，屏幕上就会出现滚动的图片，如果三列图片正好一样，那么恭喜你，Bingo！大把钞票就会涌入你的口袋，等待着你的就是迎娶BFM，走向人生巅峰......扯远了，回归到这个游戏本身。假设我们有一种**多臂**赌博机，它有不止一个握把（为了方便讨论，我们假设有三个握把），每次摇动不同握把，你会获得不一样的奖励回报，那么如何选择正确的操作方式，使得你的综合收益最高呢？有的同学就会说了，我直接三个都拉一遍，看看谁给我的奖励高我就拉黑。

恭喜你，你都会抢答了。

![Markdown](http://i1.fuimg.com/708998/2b2da3cada04902c.jpg)

那么如果我每次的奖励不一定，比如每个摇把的奖励符合不一样的**分布**呢？那我又该如何决定我的选择，使得我的收益达到最大呢？这就是**多臂赌博机问题**想要解决的问题。

## 数学角度

我们上面已经提到了所有的关键词，这里用数学符号总结一下：

* **a**——行动，也就是我们每次选择拉哪个握把的操作（action）。
* **$A_{t}$**——第t次做出的行动，通常也指我们需要求解的问题（selected action）。
* **$q^*(a)$**——行动做出后的真实奖赏，通常是不可知的，但是期望的计算结果收敛于真是奖赏（true value）。
* **$N_t(a)$**——在作出t次行动之前，行动a被选择的次数。例如，t=10，a=1指的是前9次行动中，我拉1号杆子的次数是多少次。这个指标的作用稍后会做出呈现。
* **$R_{t}$**——t次行动的实际奖赏（actual reward）。例如，在第10次选择拉1好杆以后，他给我了100块钱，这个100块就是实际奖赏$R_{t}$。

那么我们如何界定我们的总体回报呢。通常我们使用**实际平均奖赏$Q_{t}(a)$**这个指标来作为评估标准，具体计算方式如下：

$$Q_t(a)=\frac{\sum_{i=1}^{t-1}R_{i}*I_{A_{i}=a}}{N_{t}(a)}$$

解释一下，这里的$I_{A_{i}=a}$是指示函数，也就是当某次动作选择为a的时候，指示函数的值为1，否则为零。那么这里我们的分子相当于是所有以往关于a动作的奖励总和，分母是a动作的执行次数。这个指标可以很好的反应我们获得收益的**平均水平**。有了实际平均奖赏这个指标，我们就可以做点文章了。下面介绍三种经典算法。

### Greedy算法（贪婪算法）
什么叫做**贪婪算法**？就是总是选择使得当前**收益最大**的行动。对了，就是前面那位抢答的同学所说的算法。举个🌰，我们需要最大化50次摇把的收益，先不管三七二十一每个拉一遍，得到每个我把回报的初始值，之后每次都根据当次的$Q_t(a)$最大值选择拉动哪个握把。是不是很直白！！转换成公式就是：$$A_{t}=argmax_{a}Q_{t}(a)$$


部分代码如下，供大家食用（码代码不易，需要转载请q一下我）：

```
def Greedy(n, m):
    # build a list for the storing of each arm's reward
    rewards = [0]*n
    # we should initiate each arm's reward
    for i in range(n):
        rewards[i]+=probability_function(i)
    # build a list for the storing of the times of each arm that was picked
    arm_pick = [1]*n
    # build a list for the storing of the order of m times choice
    # we already got three arms at first n times
    order = [1,2,3]
    # choose the biggest one for exploration

    # already got n times arm
    for i in range(m-n):
        max_Qt = 0
        arm = 0
        for j in range(n):
            average_rewards = rewards[j]/arm_pick[j]
            if average_rewards > max_Qt:
                max_Qt = average_rewards
                # store the best choice
                arm = j
        order.append(arm+1)
        arm_pick[arm]+=1
        reward_ = probability_function(arm)
        rewards[arm]+=reward_

    print(order)
```
其中probability_function()是自己编写的不同握把的回报函数。

那么**贪婪算法**有什么好处呢？对了，就是贪婪。他每次都会尽力的挖掘已知可以挖掘的最大价值，这种特性也称作***Exploitation***（挖掘）。

### $\epsilon$-Greedy算法（$\epsilon$-贪婪算法）
什么叫做**$\epsilon$-Greedy算法**？其实就是在Greedy的基础上加上了一个$\epsilon$。这个$\epsilon$可就很灵性了，它是一个介于[0,1)的**很小的数值**，代表着每次摇动握把都有$\epsilon$的概率随机选取所有的握把。有人要问了，这有个L用啊，万一我选到别的平均收益很低的咋办？这里其实运用了另一种特性，也就是***Exploration***（探索）。这两个单词很像，但是可别弄混了奥。探索的意思是哪怕我有很大的几率会选择平均收益最高的握把，但是我还是会以小几率选择新的握把作为我的探索对象，万一他这次给的回报很高，进而超越了之前握把的平均收益呢对不对？作为对Greedy算法的改进，我也这里贴出部分代码：

```
def EGreedy(n, m):
    # build a list for the storing of each arm's reward
    rewards = [0] * n
    # we should initiate each arm's reward
    for i in range(n):
        rewards[i] += probability_function(i)
    # build a list for the storing of the times of each arm that was picked
    arm_pick = [1] * n
    # build a list for the storing of the order of m times choice
    # we already got three arms at first n times
    order = [1,2,3]
    # choose the biggest one for exploration
    # already got n times arm
    for i in range(m - n):
        max_Qt = 0
        arm = 0
        for j in range(n):
            average_rewards = rewards[j] / arm_pick[j]
            if average_rewards > max_Qt:
                max_Qt = average_rewards
                # store the best choice
                arm = j
        # epsilon was set to 0.1, so there are 1 over 10 times, we should choose arm randomly other than the profitable one
        if (i+1)%10 == 0:
            arm = np.random.randint(0,n)
        order.append(arm + 1)
        arm_pick[arm] += 1
        reward_ = probability_function(arm)
        rewards[arm] += reward_
    print(order)
```
唯一不一样的点就是for循环中添加了一个判断条件，这里我们使用的$\epsilon$是0.1，但是能不能这样分配概率仍需讨论。

### UCB算法（Upper-Confidence-Bound，上置信界）
这个算法听起来好像有点复杂奥，确实，UCB是比前两个稍微复杂一点的。那么具体复杂在哪里呢？那就是我们从简单的最大化**实际平均奖赏**这个指标，变成如下：$$At=argmax_{a}[Qt_{a}+c\sqrt{\frac{lnt}{N_{t}(a)}}]$$
因此我们需要要权衡**实际平均奖赏**和另一项（我们可以理解为**可信程度**）这两个指标。怎么理解呢，我们看到右边这一项，c是一个常数，根号里的分子是lnt，也就是当前行动次数的ln值，这对于任何一个action是一样的，而分母是行动a在t-1次中出现的次数。这个值越大，说明a被选中的越多，整个**可信程度**项就越小，那么相当于**惩罚**了选中次数较多的握把，实现了平衡。

这个算法因此更好的平衡了***Exploration***以及***Exploitation***之间的关系，贼牛！同样，算法如下：

```
def UCB(n, m, c):
    # build a list for the storing of each arm's reward
    rewards = [0] * n
    # we should initiate each arm's reward
    for i in range(n):
        rewards[i] += probability_function(i)
    # build a list for the storing of the times of each arm that was picked
    arm_pick = [1] * n
    # build a list for the storing of the order of m times choice
    # we already got three arms at first n times
    order = [1, 2, 3]
    # choose the biggest one for exploration
    # already got n times arm
    for i in range(m - n):
        max_Qt = 0
        arm = 0
        for j in range(n):
            # ucb allow us to balance exploration and exploitation
            ucb = rewards[j] / arm_pick[j] + c * np.sqrt(np.log(i+n+1)/arm_pick[j])
            if ucb > max_Qt:
                max_Qt = ucb
                # store the best choice
                arm = j
        order.append(arm + 1)
        arm_pick[arm] += 1
        reward_ = probability_function(arm)
        rewards[arm] += reward_
    print(order)
```
## 小总结
相信大家已经对多臂赌博机有所了解，这些当然只是皮毛，针对强化学习的其他内容我也会逐步更新的~睡觉！












