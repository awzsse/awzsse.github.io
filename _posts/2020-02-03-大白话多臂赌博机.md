---
layout:     post
title:      å¤§ç™½è¯å¤šè‡‚èµŒåšæœº
subtitle:   ä¸æ˜¯èµŒåšå•Šï¼Œæˆ‘æ²¡æœ‰å•Šï¼Œåˆ«ä¹±è¯´å•Šä¸‰è¿
date:       2020-02-09
author:     Noah-Yiliu
header-img: img/taitou.jpg
catalog: true
tags:
    - Reinforcement Learning
---

# å¤§ç™½è¯å¤šè‡‚èµŒåšæœº
## å¤šè‡‚èµŒåšæœºé—®é¢˜çš„é€šä¿—ç†è§£
ä»€ä¹ˆï¼Ÿ**èµŒåšæœºï¼**è¿™è·Ÿæ•°æ®åˆ†æä¸ªé¬¼çš„å…³ç³»ï¼Œè¿™ä¸å°±æ˜¯ç¢°è¿æ°”ä¹ˆã€‚å°ä¼™ä¼´ä»¬å¯åˆ«è¢«ä»–çš„å¤–è¡¨è’™è”½äº†ï¼Œè¿™å¯æ˜¯**å¼ºåŒ–å­¦ä¹ **çš„ç»å…¸é—®é¢˜ï¼Œä¹Ÿå¯ä»¥ç”¨æ¥è§£å†³ç”Ÿæ´»ä¸­çš„è¯¸å¤šé—®é¢˜ã€‚ä¸ºä»€ä¹ˆå‘¢ï¼Ÿå› ä¸ºä»–æ˜¯**å¤šè‡‚**çš„å•Šï¼Œå“ªå’ä¸å‰å®³å—ï¼Ÿ

![Markdown](http://i2.tiimg.com/708998/2629264af84d8d7b.jpg)

è¯´æ¥æƒ­æ„§ï¼Œä½œä¸ºå“è¡Œå…¼ä¼˜ç¤¾ä¼šä¸»ä¹‰å¥½é’å¹´çš„æˆ‘ä¹Ÿæ›¾ç»åœ¨è‘¡äº¬ï¼Œå·´é»äººâ€œå±å’¤é£äº‘â€ï¼ˆå’³å’³ï¼Œè‡³å°‘èµšå›äº†é…’åº—å’Œè·¯è´¹ï¼‰ã€‚ä½†æ˜¯æˆ‘ä»¬ä»Šå¤©è¦è¯´çš„ï¼Œå¯ä¸æ˜¯éª°å®ä¹‹æµï¼Œè€Œæ˜¯å¤§åé¼é¼çš„**å¤šè‡‚èµŒåšæœº**é—®é¢˜ï¼Œè‹±æ–‡å«åš***Multi Armed Bandit***ã€‚æ—¢ç„¶æ˜¯å¤§ç™½è¯ï¼Œæˆ‘ä»¬å°±ä»æœ€ç®€å•çš„è§’åº¦æ¥ç†è§£ä¸€ä¸‹ä»€ä¹ˆæ˜¯**å¤šè‡‚èµŒåšæœº**é—®é¢˜ã€‚

æƒ³å¿…å¤§å®¶éƒ½çŸ¥é“è€è™æœºé•¿å•¥æ ·å§ï¼Œå°±ç®—æ²¡ç©è¿‡ï¼Œé‚£æ²¡åƒè¿‡çŒªè‚‰ï¼Œè¿˜æ²¡è§è¿‡çŒªè·‘ä¹ˆã€‚å–ï¼Œå°±æ˜¯ä¸‹é¢è¿™ä¸ªä¸œè¥¿ã€‚

![Markdown](http://i1.fuimg.com/708998/4719084fffd891c4.jpg)

æ³¨æ„åˆ°çº¢è‰²çš„è¿™ä¸ªæ¡æŠŠäº†æ²¡ï¼Œæ¯æ¬¡å½“æˆ‘ä»¬æ‘‡ä¸‹æ¡æŠŠï¼Œå±å¹•ä¸Šå°±ä¼šå‡ºç°æ»šåŠ¨çš„å›¾ç‰‡ï¼Œå¦‚æœä¸‰åˆ—å›¾ç‰‡æ­£å¥½ä¸€æ ·ï¼Œé‚£ä¹ˆæ­å–œä½ ï¼ŒBingoï¼å¤§æŠŠé’ç¥¨å°±ä¼šæ¶Œå…¥ä½ çš„å£è¢‹ï¼Œç­‰å¾…ç€ä½ çš„å°±æ˜¯è¿å¨¶BFMï¼Œèµ°å‘äººç”Ÿå·…å³°......æ‰¯è¿œäº†ï¼Œå›å½’åˆ°è¿™ä¸ªæ¸¸æˆæœ¬èº«ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ç§**å¤šè‡‚**èµŒåšæœºï¼Œå®ƒæœ‰ä¸æ­¢ä¸€ä¸ªæ¡æŠŠï¼ˆä¸ºäº†æ–¹ä¾¿è®¨è®ºï¼Œæˆ‘ä»¬å‡è®¾æœ‰ä¸‰ä¸ªæ¡æŠŠï¼‰ï¼Œæ¯æ¬¡æ‘‡åŠ¨ä¸åŒæ¡æŠŠï¼Œä½ ä¼šè·å¾—ä¸ä¸€æ ·çš„å¥–åŠ±å›æŠ¥ï¼Œé‚£ä¹ˆå¦‚ä½•é€‰æ‹©æ­£ç¡®çš„æ“ä½œæ–¹å¼ï¼Œä½¿å¾—ä½ çš„ç»¼åˆæ”¶ç›Šæœ€é«˜å‘¢ï¼Ÿæœ‰çš„åŒå­¦å°±ä¼šè¯´äº†ï¼Œæˆ‘ç›´æ¥ä¸‰ä¸ªéƒ½æ‹‰ä¸€éï¼Œçœ‹çœ‹è°ç»™æˆ‘çš„å¥–åŠ±é«˜æˆ‘å°±æ‹‰é»‘ã€‚

æ­å–œä½ ï¼Œä½ éƒ½ä¼šæŠ¢ç­”äº†ã€‚

![Markdown](http://i1.fuimg.com/708998/2b2da3cada04902c.jpg)

é‚£ä¹ˆå¦‚æœæˆ‘æ¯æ¬¡çš„å¥–åŠ±ä¸ä¸€å®šï¼Œæ¯”å¦‚æ¯ä¸ªæ‘‡æŠŠçš„å¥–åŠ±ç¬¦åˆä¸ä¸€æ ·çš„**åˆ†å¸ƒ**å‘¢ï¼Ÿé‚£æˆ‘åˆè¯¥å¦‚ä½•å†³å®šæˆ‘çš„é€‰æ‹©ï¼Œä½¿å¾—æˆ‘çš„æ”¶ç›Šè¾¾åˆ°æœ€å¤§å‘¢ï¼Ÿè¿™å°±æ˜¯**å¤šè‡‚èµŒåšæœºé—®é¢˜**æƒ³è¦è§£å†³çš„é—®é¢˜ã€‚

## æ•°å­¦è§’åº¦

æˆ‘ä»¬ä¸Šé¢å·²ç»æåˆ°äº†æ‰€æœ‰çš„å…³é”®è¯ï¼Œè¿™é‡Œç”¨æ•°å­¦ç¬¦å·æ€»ç»“ä¸€ä¸‹ï¼š

* **a**â€”â€”è¡ŒåŠ¨ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬æ¯æ¬¡é€‰æ‹©æ‹‰å“ªä¸ªæ¡æŠŠçš„æ“ä½œï¼ˆactionï¼‰ã€‚
* **$A_{t}$**â€”â€”ç¬¬tæ¬¡åšå‡ºçš„è¡ŒåŠ¨ï¼Œé€šå¸¸ä¹ŸæŒ‡æˆ‘ä»¬éœ€è¦æ±‚è§£çš„é—®é¢˜ï¼ˆselected actionï¼‰ã€‚
* **$q^*(a)$**â€”â€”è¡ŒåŠ¨åšå‡ºåçš„çœŸå®å¥–èµï¼Œé€šå¸¸æ˜¯ä¸å¯çŸ¥çš„ï¼Œä½†æ˜¯æœŸæœ›çš„è®¡ç®—ç»“æœæ”¶æ•›äºçœŸæ˜¯å¥–èµï¼ˆtrue valueï¼‰ã€‚
* **$N_t(a)$**â€”â€”åœ¨ä½œå‡ºtæ¬¡è¡ŒåŠ¨ä¹‹å‰ï¼Œè¡ŒåŠ¨aè¢«é€‰æ‹©çš„æ¬¡æ•°ã€‚ä¾‹å¦‚ï¼Œt=10ï¼Œa=1æŒ‡çš„æ˜¯å‰9æ¬¡è¡ŒåŠ¨ä¸­ï¼Œæˆ‘æ‹‰1å·æ†å­çš„æ¬¡æ•°æ˜¯å¤šå°‘æ¬¡ã€‚è¿™ä¸ªæŒ‡æ ‡çš„ä½œç”¨ç¨åä¼šåšå‡ºå‘ˆç°ã€‚
* **$R_{t}$**â€”â€”tæ¬¡è¡ŒåŠ¨çš„å®é™…å¥–èµï¼ˆactual rewardï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨ç¬¬10æ¬¡é€‰æ‹©æ‹‰1å¥½æ†ä»¥åï¼Œä»–ç»™æˆ‘äº†100å—é’±ï¼Œè¿™ä¸ª100å—å°±æ˜¯å®é™…å¥–èµ$R_{t}$ã€‚

é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•ç•Œå®šæˆ‘ä»¬çš„æ€»ä½“å›æŠ¥å‘¢ã€‚é€šå¸¸æˆ‘ä»¬ä½¿ç”¨**å®é™…å¹³å‡å¥–èµ$Q_{t}(a)$**è¿™ä¸ªæŒ‡æ ‡æ¥ä½œä¸ºè¯„ä¼°æ ‡å‡†ï¼Œå…·ä½“è®¡ç®—æ–¹å¼å¦‚ä¸‹ï¼š

$$Q_t(a)=\frac{\sum_{i=1}^{t-1}R_{i}*I_{A_{i}=a}}{N_{t}(a)}$$

è§£é‡Šä¸€ä¸‹ï¼Œè¿™é‡Œçš„$I_{A_{i}=a}$æ˜¯æŒ‡ç¤ºå‡½æ•°ï¼Œä¹Ÿå°±æ˜¯å½“æŸæ¬¡åŠ¨ä½œé€‰æ‹©ä¸ºaçš„æ—¶å€™ï¼ŒæŒ‡ç¤ºå‡½æ•°çš„å€¼ä¸º1ï¼Œå¦åˆ™ä¸ºé›¶ã€‚é‚£ä¹ˆè¿™é‡Œæˆ‘ä»¬çš„åˆ†å­ç›¸å½“äºæ˜¯æ‰€æœ‰ä»¥å¾€å…³äºaåŠ¨ä½œçš„å¥–åŠ±æ€»å’Œï¼Œåˆ†æ¯æ˜¯aåŠ¨ä½œçš„æ‰§è¡Œæ¬¡æ•°ã€‚è¿™ä¸ªæŒ‡æ ‡å¯ä»¥å¾ˆå¥½çš„ååº”æˆ‘ä»¬è·å¾—æ”¶ç›Šçš„**å¹³å‡æ°´å¹³**ã€‚æœ‰äº†å®é™…å¹³å‡å¥–èµè¿™ä¸ªæŒ‡æ ‡ï¼Œæˆ‘ä»¬å°±å¯ä»¥åšç‚¹æ–‡ç« äº†ã€‚ä¸‹é¢ä»‹ç»ä¸‰ç§ç»å…¸ç®—æ³•ã€‚

### Greedyç®—æ³•ï¼ˆè´ªå©ªç®—æ³•ï¼‰
ä»€ä¹ˆå«åš**è´ªå©ªç®—æ³•**ï¼Ÿå°±æ˜¯æ€»æ˜¯é€‰æ‹©ä½¿å¾—å½“å‰**æ”¶ç›Šæœ€å¤§**çš„è¡ŒåŠ¨ã€‚å¯¹äº†ï¼Œå°±æ˜¯å‰é¢é‚£ä½æŠ¢ç­”çš„åŒå­¦æ‰€è¯´çš„ç®—æ³•ã€‚ä¸¾ä¸ªğŸŒ°ï¼Œæˆ‘ä»¬éœ€è¦æœ€å¤§åŒ–50æ¬¡æ‘‡æŠŠçš„æ”¶ç›Šï¼Œå…ˆä¸ç®¡ä¸‰ä¸ƒäºŒåä¸€æ¯ä¸ªæ‹‰ä¸€éï¼Œå¾—åˆ°æ¯ä¸ªæˆ‘æŠŠå›æŠ¥çš„åˆå§‹å€¼ï¼Œä¹‹åæ¯æ¬¡éƒ½æ ¹æ®å½“æ¬¡çš„$Q_t(a)$æœ€å¤§å€¼é€‰æ‹©æ‹‰åŠ¨å“ªä¸ªæ¡æŠŠã€‚æ˜¯ä¸æ˜¯å¾ˆç›´ç™½ï¼ï¼è½¬æ¢æˆå…¬å¼å°±æ˜¯ï¼š$$A_{t}=argmax_{a}Q_{t}(a)$$


éƒ¨åˆ†ä»£ç å¦‚ä¸‹ï¼Œä¾›å¤§å®¶é£Ÿç”¨ï¼ˆç ä»£ç ä¸æ˜“ï¼Œéœ€è¦è½¬è½½è¯·qä¸€ä¸‹æˆ‘ï¼‰ï¼š

```
def Greedy(n, m):
    # build a list for the storing of each arm's reward
    rewards = [0]*n
    # we should initiate each arm's reward
    for i in range(n):
        rewards[i]+=probability_function(i)
    # build a list for the storing of the times of each arm that was picked
    arm_pick = [1]*n
    # build a list for the storing of the order of m times choice
    # we already got three arms at first n times
    order = [1,2,3]
    # choose the biggest one for exploration

    # already got n times arm
    for i in range(m-n):
        max_Qt = 0
        arm = 0
        for j in range(n):
            average_rewards = rewards[j]/arm_pick[j]
            if average_rewards > max_Qt:
                max_Qt = average_rewards
                # store the best choice
                arm = j
        order.append(arm+1)
        arm_pick[arm]+=1
        reward_ = probability_function(arm)
        rewards[arm]+=reward_

    print(order)
```
å…¶ä¸­probability_function()æ˜¯è‡ªå·±ç¼–å†™çš„ä¸åŒæ¡æŠŠçš„å›æŠ¥å‡½æ•°ã€‚

é‚£ä¹ˆ**è´ªå©ªç®—æ³•**æœ‰ä»€ä¹ˆå¥½å¤„å‘¢ï¼Ÿå¯¹äº†ï¼Œå°±æ˜¯è´ªå©ªã€‚ä»–æ¯æ¬¡éƒ½ä¼šå°½åŠ›çš„æŒ–æ˜å·²çŸ¥å¯ä»¥æŒ–æ˜çš„æœ€å¤§ä»·å€¼ï¼Œè¿™ç§ç‰¹æ€§ä¹Ÿç§°ä½œ***Exploitation***ï¼ˆæŒ–æ˜ï¼‰ã€‚

### $\epsilon$-Greedyç®—æ³•ï¼ˆ$\epsilon$-è´ªå©ªç®—æ³•ï¼‰
ä»€ä¹ˆå«åš**$\epsilon$-Greedyç®—æ³•**ï¼Ÿå…¶å®å°±æ˜¯åœ¨Greedyçš„åŸºç¡€ä¸ŠåŠ ä¸Šäº†ä¸€ä¸ª$\epsilon$ã€‚è¿™ä¸ª$\epsilon$å¯å°±å¾ˆçµæ€§äº†ï¼Œå®ƒæ˜¯ä¸€ä¸ªä»‹äº[0,1)çš„**å¾ˆå°çš„æ•°å€¼**ï¼Œä»£è¡¨ç€æ¯æ¬¡æ‘‡åŠ¨æ¡æŠŠéƒ½æœ‰$\epsilon$çš„æ¦‚ç‡éšæœºé€‰å–æ‰€æœ‰çš„æ¡æŠŠã€‚æœ‰äººè¦é—®äº†ï¼Œè¿™æœ‰ä¸ªLç”¨å•Šï¼Œä¸‡ä¸€æˆ‘é€‰åˆ°åˆ«çš„å¹³å‡æ”¶ç›Šå¾ˆä½çš„å’‹åŠï¼Ÿè¿™é‡Œå…¶å®è¿ç”¨äº†å¦ä¸€ç§ç‰¹æ€§ï¼Œä¹Ÿå°±æ˜¯***Exploration***ï¼ˆæ¢ç´¢ï¼‰ã€‚è¿™ä¸¤ä¸ªå•è¯å¾ˆåƒï¼Œä½†æ˜¯å¯åˆ«å¼„æ··äº†å¥¥ã€‚æ¢ç´¢çš„æ„æ€æ˜¯å“ªæ€•æˆ‘æœ‰å¾ˆå¤§çš„å‡ ç‡ä¼šé€‰æ‹©å¹³å‡æ”¶ç›Šæœ€é«˜çš„æ¡æŠŠï¼Œä½†æ˜¯æˆ‘è¿˜æ˜¯ä¼šä»¥å°å‡ ç‡é€‰æ‹©æ–°çš„æ¡æŠŠä½œä¸ºæˆ‘çš„æ¢ç´¢å¯¹è±¡ï¼Œä¸‡ä¸€ä»–è¿™æ¬¡ç»™çš„å›æŠ¥å¾ˆé«˜ï¼Œè¿›è€Œè¶…è¶Šäº†ä¹‹å‰æ¡æŠŠçš„å¹³å‡æ”¶ç›Šå‘¢å¯¹ä¸å¯¹ï¼Ÿä½œä¸ºå¯¹Greedyç®—æ³•çš„æ”¹è¿›ï¼Œæˆ‘ä¹Ÿè¿™é‡Œè´´å‡ºéƒ¨åˆ†ä»£ç ï¼š

```
def EGreedy(n, m):
    # build a list for the storing of each arm's reward
    rewards = [0] * n
    # we should initiate each arm's reward
    for i in range(n):
        rewards[i] += probability_function(i)
    # build a list for the storing of the times of each arm that was picked
    arm_pick = [1] * n
    # build a list for the storing of the order of m times choice
    # we already got three arms at first n times
    order = [1,2,3]
    # choose the biggest one for exploration
    # already got n times arm
    for i in range(m - n):
        max_Qt = 0
        arm = 0
        for j in range(n):
            average_rewards = rewards[j] / arm_pick[j]
            if average_rewards > max_Qt:
                max_Qt = average_rewards
                # store the best choice
                arm = j
        # epsilon was set to 0.1, so there are 1 over 10 times, we should choose arm randomly other than the profitable one
        if (i+1)%10 == 0:
            arm = np.random.randint(0,n)
        order.append(arm + 1)
        arm_pick[arm] += 1
        reward_ = probability_function(arm)
        rewards[arm] += reward_
    print(order)
```
å”¯ä¸€ä¸ä¸€æ ·çš„ç‚¹å°±æ˜¯forå¾ªç¯ä¸­æ·»åŠ äº†ä¸€ä¸ªåˆ¤æ–­æ¡ä»¶ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨çš„$\epsilon$æ˜¯0.1ï¼Œä½†æ˜¯èƒ½ä¸èƒ½è¿™æ ·åˆ†é…æ¦‚ç‡ä»éœ€è®¨è®ºã€‚

### UCBç®—æ³•ï¼ˆUpper-Confidence-Boundï¼Œä¸Šç½®ä¿¡ç•Œï¼‰
è¿™ä¸ªç®—æ³•å¬èµ·æ¥å¥½åƒæœ‰ç‚¹å¤æ‚å¥¥ï¼Œç¡®å®ï¼ŒUCBæ˜¯æ¯”å‰ä¸¤ä¸ªç¨å¾®å¤æ‚ä¸€ç‚¹çš„ã€‚é‚£ä¹ˆå…·ä½“å¤æ‚åœ¨å“ªé‡Œå‘¢ï¼Ÿé‚£å°±æ˜¯æˆ‘ä»¬ä»ç®€å•çš„æœ€å¤§åŒ–**å®é™…å¹³å‡å¥–èµ**è¿™ä¸ªæŒ‡æ ‡ï¼Œå˜æˆå¦‚ä¸‹ï¼š$$At=argmax_{a}[Qt_{a}+c\sqrt{\frac{lnt}{N_{t}(a)}}]$$
å› æ­¤æˆ‘ä»¬éœ€è¦è¦æƒè¡¡**å®é™…å¹³å‡å¥–èµ**å’Œå¦ä¸€é¡¹ï¼ˆæˆ‘ä»¬å¯ä»¥ç†è§£ä¸º**å¯ä¿¡ç¨‹åº¦**ï¼‰è¿™ä¸¤ä¸ªæŒ‡æ ‡ã€‚æ€ä¹ˆç†è§£å‘¢ï¼Œæˆ‘ä»¬çœ‹åˆ°å³è¾¹è¿™ä¸€é¡¹ï¼Œcæ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œæ ¹å·é‡Œçš„åˆ†å­æ˜¯lntï¼Œä¹Ÿå°±æ˜¯å½“å‰è¡ŒåŠ¨æ¬¡æ•°çš„lnå€¼ï¼Œè¿™å¯¹äºä»»ä½•ä¸€ä¸ªactionæ˜¯ä¸€æ ·çš„ï¼Œè€Œåˆ†æ¯æ˜¯è¡ŒåŠ¨aåœ¨t-1æ¬¡ä¸­å‡ºç°çš„æ¬¡æ•°ã€‚è¿™ä¸ªå€¼è¶Šå¤§ï¼Œè¯´æ˜aè¢«é€‰ä¸­çš„è¶Šå¤šï¼Œæ•´ä¸ª**å¯ä¿¡ç¨‹åº¦**é¡¹å°±è¶Šå°ï¼Œé‚£ä¹ˆç›¸å½“äº**æƒ©ç½š**äº†é€‰ä¸­æ¬¡æ•°è¾ƒå¤šçš„æ¡æŠŠï¼Œå®ç°äº†å¹³è¡¡ã€‚

è¿™ä¸ªç®—æ³•å› æ­¤æ›´å¥½çš„å¹³è¡¡äº†***Exploration***ä»¥åŠ***Exploitation***ä¹‹é—´çš„å…³ç³»ï¼Œè´¼ç‰›ï¼åŒæ ·ï¼Œç®—æ³•å¦‚ä¸‹ï¼š

```
def UCB(n, m, c):
    # build a list for the storing of each arm's reward
    rewards = [0] * n
    # we should initiate each arm's reward
    for i in range(n):
        rewards[i] += probability_function(i)
    # build a list for the storing of the times of each arm that was picked
    arm_pick = [1] * n
    # build a list for the storing of the order of m times choice
    # we already got three arms at first n times
    order = [1, 2, 3]
    # choose the biggest one for exploration
    # already got n times arm
    for i in range(m - n):
        max_Qt = 0
        arm = 0
        for j in range(n):
            # ucb allow us to balance exploration and exploitation
            ucb = rewards[j] / arm_pick[j] + c * np.sqrt(np.log(i+n+1)/arm_pick[j])
            if ucb > max_Qt:
                max_Qt = ucb
                # store the best choice
                arm = j
        order.append(arm + 1)
        arm_pick[arm] += 1
        reward_ = probability_function(arm)
        rewards[arm] += reward_
    print(order)
```
## å°æ€»ç»“
ç›¸ä¿¡å¤§å®¶å·²ç»å¯¹å¤šè‡‚èµŒåšæœºæœ‰æ‰€äº†è§£ï¼Œè¿™äº›å½“ç„¶åªæ˜¯çš®æ¯›ï¼Œé’ˆå¯¹å¼ºåŒ–å­¦ä¹ çš„å…¶ä»–å†…å®¹æˆ‘ä¹Ÿä¼šé€æ­¥æ›´æ–°çš„~ç¡è§‰ï¼












