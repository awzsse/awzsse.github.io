---
layout:     post
title:      大白话PCA
subtitle:   降维打击！！！
date:       2020-02-29
author:     Noah-Yiliu
header-img: img/taitou.jpg
catalog: true
tags:
    - Machine Learning
---

# 大白话PCA
## 写在前面
最近更新速度有点缓慢，本来是想先写SVM相关的，但是由于SVM其中涉及到的“技术细节”实在是太多了，可能一次性也说不完。正巧前几天老师说到了PCA，虽然是一种**无监督学习**方法，于前面的**监督学习**方法的内核稍有不同，但是同样作为面试必考内容之一，我们不妨先学他一波。

由于PCA涉及到很多**“线性代数”**相关的推导，相信对很多同学来说也是个不小的门槛。那么我们这篇博客就从比较直观的角度描述一下PCA的**原理和意义**，同时忽略一些定理的证明过程，转而直接使用**定理和推论**来描述PCA的相关算法，如果后期大家有兴趣去深入了解PCA的各项定理和推论的证明过程，建议详细阅读李航老师的《统计学习方法》。当然，在介绍PCA之前，我们需要补充那么“一丢丢”的**“线代知识”**，真的只有“一丢丢”。

![锅从天上来](https://github.com/awzsse/awzsse.github.io/blob/master/img/shuaiguo.png?raw=true)

## 奇异值分解（SVD）
什么是奇异值分解呢（是不是跟奇异果🥝有关系）？这是一个线性代数的概念，我们暂时可能不能理解他的意义，先不管三七二十一看一看他的定义：
> 矩阵的奇异值分解指的是将一个非零的$m\times n$的实矩阵A进行下面的因子分解：
> 
> $$A=U\Sigma V^{T}$$
> 
> 其中$U, V$分别为$m, n$阶正交矩阵，$\Sigma$是降序排列的非负的对角线元素组成的对角矩阵，维度为$m\times n$

头疼吗？头疼就对了。首先不清楚什么是正交矩阵，对角矩阵的同学请自行翻阅百度百科。简单来说，正交矩阵就是矩阵中的每个**列向量**的点乘结果为零（可以想象成x和y坐标轴单位向量的相乘），而对角阵，就是[对角阵](https://baike.baidu.com/item/%E5%AF%B9%E8%A7%92%E7%9F%A9%E9%98%B5)啊！！

那么有同学会问：***“随便给个实数矩阵，他的奇异值分解一定存在吗？”***。你说的没错，并且，他是一条定理，记住他！

这里要注意的是，上述定义定义的是**完全奇异值分解**。实际上，我们经常使用的是以下两种形式：

### 紧奇异值分解
> 矩阵的“紧奇异值分解”指的是对于一个非零的$m\times n$的实矩阵A，其秩为$rank(A)=r,r\leq min(m,n)$进行下面的因子分解：
> 
> $$A=U_{r}\Sigma_{r} V_{r}^{T}$$
> 
> 其中$U_{r}, V_{r}$分别为$m\times r, n\times r$矩阵，$\Sigma$是降序排列的非负的对角线元素组成的$r$阶对角矩阵。

那么这个矩阵对分解后的因子做出了一些限制，其中$U$是由完全奇异值分解中$U$的前r列组成的，$V$是由$V$的前r列组成的。从形态上来看，如果没有限制的话，原本$U, V$矩阵的列可以超过r这个数值，然而这种限制使得这三个分解的因子都变得更加**紧凑**了对不？但是他还只是**紧凑**，不会过分小。

### 截断奇异值分解

> 矩阵的“截断奇异值分解”指的是对于一个非零的$m\times n$的实矩阵$A$，其秩为$rank(A)=r$取$0<k<r$进行下面的因子分解：
> 
> $$A\approx U_{k}\Sigma_{k} V_{k}^{T}$$
> 
> 其中$U_{k}, V_{k}$分别为$m\times k, n\times k$矩阵，$\Sigma$是降序排列的非负的对角线元素组成的$k$阶对角矩阵。

明显看出，这个分解出来的几个因子就更**紧凑**，不，甚至被**截断**了。注意到，这里用的是约等于，也就是说这个因子分解回算结果并不是原本的$A$，而只是一个近似值。在实际应用中我们可以将其理解为一种对数据的压缩，给出一个差不多的，但是有些损失的结果。

### 几何解释，非常香
为什么要这样分解呢。原本的$A$是有什么不好的地方吗？我们从线性变换的角度去理解一下奇异值分解。首先这个$m\times n$的矩阵$A$表示一个从n维空间到m维空间的线性变换，也就是：

$$T: x\rightarrow Ax$$

怎么理解呢，也就是说我$x$原本是一个n维的向量，通过$A$这个$m\times n$的矩阵变换一下，变成了$Ax$，也就是一个$m$维的向量。而这整个过程可以通过下图表示：

![SVD](https://github.com/awzsse/awzsse.github.io/blob/master/img/SVD.jpg?raw=true)

因此，$A$看成一种变换的话，其同等结果的三个因子相乘肯定也是同样的变换。由于$Ax$中$x$是右乘的，因此我们的三个因子的作用也是从右往左。首先是$V^{T}$的一个坐标系的旋转或者反射，$\Sigma$进行了坐标轴的缩放而$U$又是旋转或者反射才得到了最后的结果。

实际上关于奇异值分解还有很多性质，这里我们不再赘述。

### 奇异值分解的计算
那么说了这么多，都是建立在我们已经分解完成的基础上。那么具体怎么进行分解呢，我们通过一个实例来给大家叙述一下基本步骤，上手抄版本！

![](https://github.com/awzsse/awzsse.github.io/blob/master/img/svd.jpg?raw=true)

这次感觉字倒不是问题，问题是我眼睛好像是斜的。

不论如何，上述4个步骤按照步骤求解出三个分解因子。

### 奇异值分解与矩阵近似
回忆刚才的截断矩阵，其实就一种矩阵的近似方式，而这个近似是在**弗罗贝尼乌斯范数**的意义下的近似。
> 弗罗贝尼乌斯范数：假设矩阵$A_{m\times n}$，其弗罗贝尼乌斯范数为
> 
> $$||A||_{F}=(\sum_{i=1}^{m}\sum_{i=1}^{n}(a_{ij}^2))^{\frac{1}{2}}$$

可以看出，这个范数类似于一种向量范数的引申，同时给出一条引理用于计算矩阵的范数。

> $$||A||_{F}=(\sigma_{1}^{2}+\sigma_{2}^{2}+...+\sigma_{n}^{2})^{\frac{1}{2}}$$
> 
> 其中$\sigma$是按照降序排列的矩阵$A$奇异值分解中对角阵的对角线元素值

那么什么是最优近似呢？关键来了！！！！我们前面叙述了这么多定义和原理，是为了什么？结合之前所有的铺垫，我们总结一句话：

***奇异值分解就是在平方损失（弗罗贝尼乌斯范数）意义下矩阵的最优近似。***

PCA是一个什么过程？实际上就是通过对数据的降维来更好的分析数据的过程（具体后面再说），那么这里降维怎么实现才可以达到对数据最大程度的保留，其实就是矩阵的最优化近似问题，我们假设矩阵$A$的秩为$r$，$M$为$m\times n$矩阵中所有的秩不超过$k$的矩阵集合，同时满足$0<k<r$。这时候我们如果能找到一个矩阵$X$，使得

$$||A-X||_{F}=min_{S\in M}||A-S||$$

那么$X$就是压缩后的$A$但是又保证了最大的信息保留。那么这个最小的**平方损失**计算方法为

$$||A-X||_{F}=(\sigma_{k+1}^{2}+\sigma_{k+2}^{2}+...+\sigma_{n}^{2})^{\frac{1}{2}}$$

剩下就是如何**求解**这个**最优近似$X$矩阵**了，我们使用的方法是矩阵的**外积展开式**。为啥叫展开呢，我们来看看他的具体形式

$$A=\sum_{k=1}^{n}A_{k}=\sum_{k=1}^{n}\sigma_{k}\mu_{k}v_{k}^{T}$$

具体证明就不赘述了，也就是把奇异值分解后的因子中正交矩阵列向量以及对角矩阵中的对角元素提出来组合一下~那么用一个具体的例子表示计算的过程，再次上手抄板！！！！

![](https://github.com/awzsse/awzsse.github.io/blob/master/img/%E5%A4%96%E7%A7%AF.png?raw=true)

图中红色部分表示我们经过截断奇异值分解以后保留的信息，其实也就是秩为2的矩阵中在弗罗贝尼乌斯范数下$A$的最优近似矩阵！！

## PCA
这个铺垫着实有点长了，但是说清楚奇异值分解以后，关于后文中提到的利用“奇异值分解计算PCA主成分”就会显得顺理成章。PCA是什么呢？他的全称叫做主成分分析，这是一种常用的无监督学习方式。复习一下，也就是针对一群没有label数据的处理方法。我们分别通过**总体**和**样本**的主成分分析来展开叙述。需要注意的是，当时笔者在理解总体主成分分析的时候犯了一个错误，导致很多概念模糊不清，那就是总体主成分分析是针对随机变量**$x$**的分析，也就是说原本的随机变量进行降维后变成**另外的**一批随机变量，这里没有涉及到对某个具体**数据**的处理，希望大家也要注意这一个点。

### 总体主成分分析
我们首先来说说PCA有啥用吧。我们知道在统计分析中，数据的变量之间会存在一些**相关性**。举个例子，身高和体重是两个随机变量，那么很有可能是身高越高，体重越重，那么如果我们将这两个维度的变量放到某个模型中拟合可能会增加分析的难度。**PCA**的主要思想就是，通过对原始变量表示的数据进行**正交变换**，把他们变成又**新的**，**线性无关**的变量表示的数据，而一般来说根据新变量的方差（也就是保留原始信息的程度）决定最主要的几个**“成分”**，所以新变量的维度一般会小于原始的变量，所以PCA也达到了降维的作用。

这段话听起来可能有点拗口，下面我们用数学表达的方式给大家详细说明其定义。

#### 总体主成分定义
首先假设$x=(x_{1},x_{2},...,x_{m})^{T}$是m维的随机变量，记住，这里**x**是随机变量，不是说是某个数据。设其均值向量是$\mu$，协方差矩阵是$\Sigma$，考虑到由$m$维随机变量**$x$**到m维随机变量**$y=(y_{1},y_{2},...,y_{m})^{T}$**的线性变换，则：

$$y_{i}=\alpha_{i}^{T}x=\alpha_{1i}x_{1}+\alpha_{2i}x_{2}+...+\alpha_{mi}x_{m}$$

这里$\alpha_{i}^{T}=(\alpha_{1i},\alpha_{2i},...,\alpha_{mi}),i=1,2,...,m$。因此我们看出针对每个$y_{i}$，我们都要用到之前每个维度的信息，这样可以得到m个新的维度。根据随机变量的性质我们可以得到以下几个参数：

$$E(y_{i})=\alpha_{i}^{T}\mu$$

$$var(y_{i})=\alpha_{i}^{T}\Sigma \alpha_{i}$$

$$cov(y_{i},y_{j})=\alpha_{i}^{T}\Sigma \alpha_{j}$$

那么如果给定以上线性变换，他们满足（1）系数向量是$\alpha_{i}^{T}$单位向量（2）$y_{i}，y_{j}，i\neq j$的协方差为零（3）$y_{1}$是所有线性变换中方差最大的，也就是$var(y_{1})$是最大的，$y_{2}$是与$y_{1}$不相关的所有线性变换中方差最大的，以此类推，则$y_{1},...,y_{m}$就是$x$的第一主成分，第二主成分，...，第m主成分。

#### 性质和求法
由于我们已经知道什么是主成分了，下面通过一个**定理**和一个**推论**来总结一下主成分的求解方法以及相关性质。

> 假设**$x$**是$m$维随机变量，$\Sigma$是其协方差矩阵，它的特征值分别为$\lambda_{1}\geq \lambda_{2}\geq ... \geq \lambda_{m} \geq 0$，特征值对应的单位特征向量分别是$\alpha_{1}，\alpha_{2}，...，\alpha_{m}$，那么$x$的第k主成分是
> 
> $$y_{k}=\alpha_{k}^{T}x=\alpha_{1k}x_{1}+\alpha_{2k}x_{2}+...+\alpha_{mk}x_{m}$$
> 
> 而$x$的第k主成分的方差为
> 
> $$var(y_{k})=\alpha_{k}^{T}\Sigma\alpha_{k}$$

根据前面提到的主成分定义，$\lambda$的递减代表着方差的递减。

那么另一个推论从另一个角度理解了主成分。

> $y=(y_{1},y_{2},...,y_{m})^{T}$的分量依次是$x$的第$1-m$主成分的充分必要条件为:
> 
> $$y=A^{T}x$$
> 
> 其中$A$为正交矩阵，他的列向量分别是原始数据协方差矩阵的单位特征向量。同时$y$的协方差矩阵还要是一个对角矩阵：
> 
> $$cov(y)=diag(\lambda_{1}，\lambda_{2}，...，\lambda_{m})$$

这里的$\lambda$是不是很熟悉，没错，这就是原始数据协方差矩阵的特征值。因此我们可以将推论看做是定理的另一种表达形式。

那么我们总结一下总体主成分的几个性质：

* 首先总体主成分的协方差矩阵是一个对角阵
* 总体主成分的方差之和等于随机变量**$x$**的方差之和（表明数据信息不会丢失），即

$$\sum_{i=1}^{m}\lambda_{i}=\sum_{i=1}^{m}\sigma_{ii}$$

* 第$k$个主成分和变量的**相关系数**称为**因子负荷量**，其计算方法为：

$$\rho(y_{k},x_{i})=\frac{\sqrt{\lambda_{k}}\alpha_{ik}}{\sigma_{ii}}，k=1,2,...m$$

* 第$k$个主成分和$m$个变量的因子负荷量满足如下等式：

$$\sum_{i=1}^{m}\sigma_{ii}\rho^2(y_{k},x_{i})=\lambda_{k}$$

* 而$m$个主成分和第$i$各变量的因子负荷量满足：

$$\sum_{k=1}^{m}\rho^{2}(y_{k},x_{i})=1$$

#### 方差贡献率-主成分选择
那么我们已经知道了所有主成分的求法，我们该如何选择我们需要的$k$值呢？也就是我们如何选择我们保留几个主成分呢，有一个判别指标叫做**方差贡献率**，定义为某个主成分$y_{k}$的方差$\lambda_{k}$和所有方差和的比。而前$k$个**方差贡献率**的和，也就是**累计方差贡献率**。例如，我们可以定义一个阈值为80%，那么我们就选可以让累计方差贡献率达到80%以上的**$k$**值就好。

不过**累计方差贡献率**无法反应对**某个原有变量**的贡献率，这种贡献率可以用不同主成分与某个变量相关系数的平方和来表示：

$$v_{i}=\sum_{i=1}^{k}\rho^2(x_{i},y_{i})=\sum_{i=1}^{k}\frac{\lambda_{j}\alpha^2_{ij}}{\sigma_{ii}}$$

这里讨论他的合理性，为什么可以这样计算呢？还记得之前我们提到的最后一个性质吗？也就是$m$个主成分因子负荷量的平方和满足总和为1，那么如果我选了$k$个，那么这个值一定是一个小于1的值，可以形象的理解成一个“置信度”。

### 样本主成分分析
我们已经搞清楚总体主成分分析的方法了，但是在实际问题中我们往往需要针对具体的数据集进行操作，这也就涉及到**样本**的主成分分析过程。这里唯一需要注意的一点是：

* 样本协方差矩阵$S$是总体协方差矩阵$\Sigma$的无偏估计，当样本规范化以后，其协方差矩阵就变成了相关系数矩阵$R$，具体表达方式如下：

$$R=\frac{1}{n-1}XX^{T}$$

而其余特性与总体主成分分析类似，这里就不赘述。主要提及的是以下两个主成分计算方法。

#### ——相关矩阵的特征值分解算法
* 将观测数据进行规范化处理，以**$X$**表示
* 计算样本的相关矩阵$R$
* 求解$R$的相关矩阵$|R-\lambda I|=0$，从而得到$m$个特征值，根据方差贡献率选择主成分个数为$k$，再求出$k$个特征向量。
* 通过$k$个特征向量$\alpha_{i}$计算$k$个主成分（线性变换$y_{i}=\alpha{i}^{T}x$）

#### ——数据矩阵的奇异值分解算法
* 构造一个新的矩阵$X'_{n\times m}$:

$$X'=\frac{1}{\sqrt{n-1}}X^{T}$$ 

具体为什么这样构造是为了使得：

$$S = X'^{T}X'$$

* 对矩阵$X'$进行截断奇异值分解，得到：

$$X'=U\Sigma V^{T}$$ 

终于和SVD联系在一起了，撒花。这里我们通过求解$X'$的奇异值分解，可以知道矩阵$V$的💰$k$列就构成了上一个步骤中提到的，样本协方差的特征向量，因此可以利用这些特征向量求出前$k$个样本主成分。

* $$Y=V^{T}X$$

## 写在后面
其实整个PCA的过程就这么简单，如果不考虑定理的推导等步骤，我们只需要知道通过某个线性变换过程，可以将原始的数据维度降低，在新的维度也就是主成分下，我们可以更好地分析数据同时保证数据的信息被大部分保留。






