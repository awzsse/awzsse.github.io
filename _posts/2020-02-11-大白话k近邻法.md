---
layout:     post
title:      大白话k近邻法
subtitle:   我们离得近，所以我们。。。。。
date:       2020-02-11
author:     Noah-Yiliu
header-img: img/taitou.jpg
catalog: true
tags:
    - Machine Learning
---

# 大白话k近邻法
## 写在前面
咳咳，小葵花妈妈课堂开课了，最近咳嗽老不好，一定是博客写少了，写多几篇就好了。耽误这么多天，终于决定静下心来好好把上学期的学习内容总结一下。结合之前发布的**大白话感知机**，在未来一段时间我将陆续更新机器学习的几个经典算法，同时穿插一些数学、计算机方面的知识补充，为春招和秋招打好基础！来，老板，准备🥝。
## k近邻法的通俗解释
什么是**k邻近法**呢？最近香港惠康最好吃的猕猴桃🥝找不到了，可能因为太好吃了，被抢厕纸的那帮人抢完了吧=。=，但是我还是要拿他举例！

假设你又要对一堆猕猴桃进行分类了（回想一下当时我们对[感知机](https://awzsse.github.io/2020/01/22/%E5%A4%A7%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95-%E6%84%9F%E7%9F%A5%E6%9C%BA/)的理解），我们需要判断其果蒂，毛色等指标，根据这些空间中的**特征向量**来输出针对猕猴桃口感好坏的判断。然而不一样在哪呢？在感知机中，我们通过划分一个**超平面**将两种口感的猕猴桃分开，是不是有点简单粗暴了？万一这个数据是**线性不可分**的怎么办，你给我画个看看。

![pi](https://github.com/awzsse/awzsse.github.io/blob/master/img/pi.jpg?raw=true)

有同学要问了，那这咋办啊。我总不能画曲线吧。。其实可以，不过这不是k近邻法的本质。我们知道好的猕猴桃🥝往往具有一些共性，也就是他们的**特征**可能是类似的，只是我们该怎么界定这种类似呢？

这里小总结一下，我们已知了一群猕猴桃是好吃的，另一群是不好吃的，这些猕猴桃构成了**训练集**，现在的目的是，给我一些未知是不是好吃的猕猴桃，也就是**测试集**，我们需要对其进行正确的分类。问题在哪呢？不能用感知机，因为可能存在**线性不可分**的情况，即特征不是非黑即白的，可能毛色亮一点的和毛色暗一点的都存在好吃的可能。我们这时候希望通过与已知猕猴桃对比，找出**综合特征**类似的给其赋予相应的标签。类似是不是说明他们很“近”呢？对！如果我可以把特征数字化，找出离目标猕猴桃“距离”最近的猕猴桃，那他们肯定口感类似哇。

ok，至此，我们已经完全引入了k近邻法的两个基本要素：**距离度量**，**k值**。另一个要素是**分类决策规则**将随后提出。

## k近邻法的数学解释
### 距离度量
什么是**距离度量**？我们知道，每一个猕猴桃其实就是**特征空间**中的**实例点**，习惯上，对于n维实数向量空间的点，我们使用**欧式距离**来表示他们的距离。别迷糊了，二维空间中就是两点之间的距离公式，这总不能忘记吧。具体的：$$L_{2}(x_{i},x{j})=(\sum_{l=1}^{n}|x_{i}^{(l)}-x_{j}^{(l)}|^{2})^{\frac{1}{2}}$$
当然也还有著名的[曼哈顿距离](https://baike.baidu.com/item/%E6%9B%BC%E5%93%88%E9%A1%BF%E8%B7%9D%E7%A6%BB)，只是没有欧氏距离常用。也就是说，我们可以把猕猴桃各个特征之间的距离用欧式距离来表示，离我们想确定分类的猕猴桃**越近**的个体，就越有话语权。

### k值
k邻近法k邻近法，什么是k值？很简单，就是你要选离目标个体**最近的几个**猕猴桃作为评判标准，而**k**就是你要选的个数。在这里，k值的选取显得非常重要。如果k值过大，有一种天下乌鸦一般黑的感觉，所有猕猴桃仿佛都被你一视同仁了，你无法做出良好的区分。那么有的同学说，直接设置最小，k=1，我就看看最近的那个猕猴桃是好吃还是难吃不就完了，这样很有说服力啊！

![zhuangbi](https://github.com/awzsse/awzsse.github.io/blob/master/img/zhuangbi.jpg?raw=true)

其实这位同学说的有一定道理，但是这样做容易造成模型的**过拟合**，也就是整个模型变得非常复杂，预测结果会对邻近的实例点非常敏感，这也不是办法。

通常，我们会选取一个**较小**的值。

### 分类决策规则
在已知了距离度量以及k值后，我们需要关注最后一个问题。如何分类呢？这个叫“有的”的同学又出来了，不就是按照最近的猕猴桃好不好吃判断吗？那我想问问你，万一最近的有好有坏呢。你给他什么标签合适呢？我们来看下面这张图：
![knn](https://github.com/awzsse/awzsse.github.io/blob/master/img/kjinlin.jpg?raw=true)

这里k值是3，因为是选取了三个最近的点，距离度量肯定是欧氏距离啦，那么三个点中有两个是橙色的，一个是蓝色的。这时，我们通过**多数表决规则*****（Majority Voting Rule）***将目标点分类为蓝色。
>多数表决规则：$y=argmax_{c_{j}}\sum_{x_{i}\in N_{k}(x)}I(y_{i}=c_{j}), i=1,2,...N;j=1,2,...K$

这是李航老师《统计学习方法》第二版这本书P50页给出的定义，简单解释就是目标点的分类结果由其邻近的k个点中较多数的点决定，这也符合[经验风险最小化](https://zh.wikipedia.org/wiki/%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A9%E6%9C%80%E5%B0%8F%E5%8C%96)的要求。

## k近邻法的实现思路
有关机器学习经典方法的代码实现我将后续推出，大部分灵感来自于大神Dodo，在k近邻方法中，比较关键的点是如何实现**最近k个点的选取**。大致思路为：

* 将目标点与训练集中所有点的距离计算出来
* 将距离排序，选出前k个即可

博主暂时只实现了以上简单的k近邻算法，也称为**线性扫描**方法，以后会通过一篇单独的博客介绍一种训练集很大时运用的算法：**kd树**。

## 总结
除了kd树，k近邻方法大部分的精髓我已经传授给大家了，满足的躺下！
 








